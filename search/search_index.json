{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"MCAP (pronounced \u201cem-cap\u201d) is a modular container file format for heterogeneous timestamped data. It is ideal for robotics applications, as it can record multiple streams of structured and unstructured data (e.g. ROS, Protobuf, JSON Schema, etc.) in a single file. Benefits MCAP works well under various workloads, resource constraints, and durability requirements. Heterogeneous data Store messages encoded in multiple serialization formats in a single file Include metadata and attachments Performant writing Append-only structure Recover partially-written files when data recording is interrupted Efficient seeking Extract data without scanning the entire file Fast access to indexed summary data Self-contained files Embed all message schemas in the file No extra dependencies needed for decoding History Before MCAP Many robotics companies spend valuable in-house resources to develop custom file formats, only to create future work and complicate third-party tool integrations. We built MCAP to allow teams to focus on their core robotics problems and avoid wasting precious time making commodity tools. Before MCAP, the format that robotics teams used to store their log data depended mainly on their framework. Those using ROS 1 defaulted to the \u201cbag file\u201d format; those on ROS 2 defaulted to a SQLite-based format. Companies that don\u2019t use ROS often employed a custom in-house binary format, such as length-delimited Protobuf, or stored their messages as opaque bytes inside existing file formats such as HDF5. These existing storage options have several shortcomings. Custom in-house formats lack interoperability and require developing corresponding libraries in multiple languages to read and write files. The ROS 1 bag format is challenging to work with outside of the ROS ecosystem, while the ROS 2 SQLite format is not fully self-contained , making it difficult for third-party tools to read. After MCAP We evaluated many existing data storage formats in the industry and identified a clear need for a general-purpose, open source data container format \u2013 specifically optimized for robotics use cases. Designing this format would solve an industry-wide problem and make it easier for teams to leverage third-party tools and share their own tooling. As a container format, MCAP solves many of these issues. It is self-contained, can embed multiple data streams encoded with different serialization formats in a single file, and even supports metadata and attachments. MCAP files are optimized for both high-performance writing and efficient indexed reading, even over remote connections.","title":"Introduction"},{"location":"index.html#benefits","text":"MCAP works well under various workloads, resource constraints, and durability requirements.","title":"Benefits"},{"location":"index.html#heterogeneous-data","text":"Store messages encoded in multiple serialization formats in a single file Include metadata and attachments","title":"Heterogeneous data"},{"location":"index.html#performant-writing","text":"Append-only structure Recover partially-written files when data recording is interrupted","title":"Performant writing"},{"location":"index.html#efficient-seeking","text":"Extract data without scanning the entire file Fast access to indexed summary data","title":"Efficient seeking"},{"location":"index.html#self-contained-files","text":"Embed all message schemas in the file No extra dependencies needed for decoding","title":"Self-contained files"},{"location":"index.html#history","text":"","title":"History"},{"location":"index.html#before-mcap","text":"Many robotics companies spend valuable in-house resources to develop custom file formats, only to create future work and complicate third-party tool integrations. We built MCAP to allow teams to focus on their core robotics problems and avoid wasting precious time making commodity tools. Before MCAP, the format that robotics teams used to store their log data depended mainly on their framework. Those using ROS 1 defaulted to the \u201cbag file\u201d format; those on ROS 2 defaulted to a SQLite-based format. Companies that don\u2019t use ROS often employed a custom in-house binary format, such as length-delimited Protobuf, or stored their messages as opaque bytes inside existing file formats such as HDF5. These existing storage options have several shortcomings. Custom in-house formats lack interoperability and require developing corresponding libraries in multiple languages to read and write files. The ROS 1 bag format is challenging to work with outside of the ROS ecosystem, while the ROS 2 SQLite format is not fully self-contained , making it difficult for third-party tools to read.","title":"Before MCAP"},{"location":"index.html#after-mcap","text":"We evaluated many existing data storage formats in the industry and identified a clear need for a general-purpose, open source data container format \u2013 specifically optimized for robotics use cases. Designing this format would solve an industry-wide problem and make it easier for teams to leverage third-party tools and share their own tooling. As a container format, MCAP solves many of these issues. It is self-contained, can embed multiple data streams encoded with different serialization formats in a single file, and even supports metadata and attachments. MCAP files are optimized for both high-performance writing and efficient indexed reading, even over remote connections.","title":"After MCAP"},{"location":"support-matrix.html","text":"Library Features and Support Matrix Library Support The Python, C++, Go, and TypeScript MCAP libraries are actively developed. This means that Foxglove actively pursues bug fixes and ensures conformance with the MCAP specification. Note : This does not mean that their APIs are stable. The Swift MCAP library is experimental, and not actively developed. This means that PRs contributing bug-fixes are welcome, but GitHub Issues regarding it will not be prioritized. Feature Matrix Python C++ Go TypeScript Swift Rust Indexed unordered message reading Yes Yes Yes Yes No Yes Timestamp-ordered message reading Yes Yes Yes Yes Yes No Indexed metadata reading Yes Yes 1 Yes 1 Yes 1 Yes 1 Yes Indexed attachment reading Yes Yes 1 Yes 1 Yes 1 Yes 1 Yes Non-materialized attachment reading No Yes 2 No No No Yes Non-indexed reading Yes Yes Yes Yes Yes Yes CRC validation No No Yes Yes Yes Yes ROS1 wrapper Yes No No No No No ROS2 wrapper Yes 3 Yes 3 No No No No Protobuf wrapper Yes No No No No No Record writing Yes Yes Yes Yes Yes Yes Easy chunked writing Yes Yes Yes Yes Yes Yes Automatic summary writing Yes 4 Yes 4 Yes 4 Yes 4 Yes 4 Yes These readers don\u2019t have a single call to read an attachment or metadata record by name, but do allow you to read the summary, seek to that location, read a record and parse it. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Using the MCAP Rosbag2 storage plugin . \u21a9 The C++ reader interface does not preclude one from backing it with a memory-mapped file. This could be used to implement message and attachment parsing without copying data into memory. \u21a9 \u21a9 All writers currently do not compute a CRC for the DataEnd record. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Library Support"},{"location":"support-matrix.html#library-features-and-support-matrix","text":"","title":"Library Features and Support Matrix"},{"location":"support-matrix.html#library-support","text":"The Python, C++, Go, and TypeScript MCAP libraries are actively developed. This means that Foxglove actively pursues bug fixes and ensures conformance with the MCAP specification. Note : This does not mean that their APIs are stable. The Swift MCAP library is experimental, and not actively developed. This means that PRs contributing bug-fixes are welcome, but GitHub Issues regarding it will not be prioritized.","title":"Library Support"},{"location":"support-matrix.html#feature-matrix","text":"Python C++ Go TypeScript Swift Rust Indexed unordered message reading Yes Yes Yes Yes No Yes Timestamp-ordered message reading Yes Yes Yes Yes Yes No Indexed metadata reading Yes Yes 1 Yes 1 Yes 1 Yes 1 Yes Indexed attachment reading Yes Yes 1 Yes 1 Yes 1 Yes 1 Yes Non-materialized attachment reading No Yes 2 No No No Yes Non-indexed reading Yes Yes Yes Yes Yes Yes CRC validation No No Yes Yes Yes Yes ROS1 wrapper Yes No No No No No ROS2 wrapper Yes 3 Yes 3 No No No No Protobuf wrapper Yes No No No No No Record writing Yes Yes Yes Yes Yes Yes Easy chunked writing Yes Yes Yes Yes Yes Yes Automatic summary writing Yes 4 Yes 4 Yes 4 Yes 4 Yes 4 Yes These readers don\u2019t have a single call to read an attachment or metadata record by name, but do allow you to read the summary, seek to that location, read a record and parse it. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Using the MCAP Rosbag2 storage plugin . \u21a9 The C++ reader interface does not preclude one from backing it with a memory-mapped file. This could be used to implement message and attachment parsing without copying data into memory. \u21a9 \u21a9 All writers currently do not compute a CRC for the DataEnd record. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Feature Matrix"},{"location":"getting-started/json.html","text":"JSON Read and write MCAP If you\u2019re starting from scratch, you can write code that allows you to write your JSON data to MCAP files and subsequently read your JSON data from your MCAP files . Guides Python Examples Python - CSV to MCAP converter C++ - reader , writer Inspect MCAP Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo . Visualize MCAP Foxglove Studio supports playing back local and remote MCAP files containing JSON data.","title":"JSON"},{"location":"getting-started/json.html#json","text":"","title":"JSON"},{"location":"getting-started/json.html#read-and-write-mcap","text":"If you\u2019re starting from scratch, you can write code that allows you to write your JSON data to MCAP files and subsequently read your JSON data from your MCAP files .","title":"Read and write MCAP"},{"location":"getting-started/json.html#guides","text":"Python","title":"Guides"},{"location":"getting-started/json.html#examples","text":"Python - CSV to MCAP converter C++ - reader , writer","title":"Examples"},{"location":"getting-started/json.html#inspect-mcap","text":"Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo .","title":"Inspect MCAP"},{"location":"getting-started/json.html#visualize-mcap","text":"Foxglove Studio supports playing back local and remote MCAP files containing JSON data.","title":"Visualize MCAP"},{"location":"getting-started/overview.html","text":"Overview You may be interested in using MCAP for a variety of scenarios throughout your robotics development workflows. Convert to MCAP If you already have existing ROS data that is not in the MCAP file format, you may want to convert it into MCAP files . The easiest way to do this is to install the MCAP CLI tool and use it to convert your existing bag files . Read and write MCAP If you\u2019re starting from scratch, you may want to write code to read and write your own MCAP data . We provide MCAP readers and writers in the following languages, for any of our supported data serialization formats : Python C++ Go Swift TypeScript Rust Inspect and visualize MCAP Once you have MCAP data to work with, you may want to use the mcap CLI to inspect and interact with your MCAP files . You can also start visualizing your MCAP data with third-party tools like Foxglove Studio . To start leveraging all the visualizations Studio has to offer, you must write messages that adhere to a pre-defined set of Foxglove schemas. The @foxglove/schemas repo provides pre-defined schemas in the following data serialization formats: ROS 1 ROS 2 JSON Schema Protobuf TypeScript Next, download Foxglove Studio as a desktop app , or navigate to studio.foxglove.dev in your web browser. For local MCAP files , simply drag and drop the file into the Studio app to start playing back your data. For remote MCAP files , select \u201cOpen file from URL\u201d when you first load the Studio app, and specify the URL to your remote MCAP file. Open the connection to start playing back your data. Add and configure different panels to your layout to visualize different aspects of your data. Store MCAP Sign up for a Foxglove account to access the data management features of Foxglove Data Platform . Instead of having to pass around hard drives every time you want to share data, you can start importing your MCAP data into a central repository for all your teammates to access and collaborate on. Instead of having to download enormous files to your local machine whenever you want to inspect some data, you can stream your team data directly to whatever environment you\u2019d like for further analysis.","title":"Overview"},{"location":"getting-started/overview.html#overview","text":"You may be interested in using MCAP for a variety of scenarios throughout your robotics development workflows.","title":"Overview"},{"location":"getting-started/overview.html#convert-to-mcap","text":"If you already have existing ROS data that is not in the MCAP file format, you may want to convert it into MCAP files . The easiest way to do this is to install the MCAP CLI tool and use it to convert your existing bag files .","title":"Convert to MCAP"},{"location":"getting-started/overview.html#read-and-write-mcap","text":"If you\u2019re starting from scratch, you may want to write code to read and write your own MCAP data . We provide MCAP readers and writers in the following languages, for any of our supported data serialization formats : Python C++ Go Swift TypeScript Rust","title":"Read and write MCAP"},{"location":"getting-started/overview.html#inspect-and-visualize-mcap","text":"Once you have MCAP data to work with, you may want to use the mcap CLI to inspect and interact with your MCAP files . You can also start visualizing your MCAP data with third-party tools like Foxglove Studio . To start leveraging all the visualizations Studio has to offer, you must write messages that adhere to a pre-defined set of Foxglove schemas. The @foxglove/schemas repo provides pre-defined schemas in the following data serialization formats: ROS 1 ROS 2 JSON Schema Protobuf TypeScript Next, download Foxglove Studio as a desktop app , or navigate to studio.foxglove.dev in your web browser. For local MCAP files , simply drag and drop the file into the Studio app to start playing back your data. For remote MCAP files , select \u201cOpen file from URL\u201d when you first load the Studio app, and specify the URL to your remote MCAP file. Open the connection to start playing back your data. Add and configure different panels to your layout to visualize different aspects of your data.","title":"Inspect and visualize MCAP"},{"location":"getting-started/overview.html#store-mcap","text":"Sign up for a Foxglove account to access the data management features of Foxglove Data Platform . Instead of having to pass around hard drives every time you want to share data, you can start importing your MCAP data into a central repository for all your teammates to access and collaborate on. Instead of having to download enormous files to your local machine whenever you want to inspect some data, you can stream your team data directly to whatever environment you\u2019d like for further analysis.","title":"Store MCAP"},{"location":"getting-started/protobuf.html","text":"Protobuf Read and write MCAP If you\u2019re starting from scratch, you can write code that allows you to write your Protobuf data to MCAP files and subsequently read your Protobuf data from your MCAP files . The mcap GitHub repo includes the mcap-protobuf-support Python package to help you write an MCAP reader and writer for Protobuf. Guides Python C++ Examples Python - reader and writer C++ - dynamic reader , static reader , and writer Inspect MCAP Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo . Visualize MCAP Foxglove Studio supports playing back local and remote MCAP files containing Protobuf data.","title":"Protobuf"},{"location":"getting-started/protobuf.html#protobuf","text":"","title":"Protobuf"},{"location":"getting-started/protobuf.html#read-and-write-mcap","text":"If you\u2019re starting from scratch, you can write code that allows you to write your Protobuf data to MCAP files and subsequently read your Protobuf data from your MCAP files . The mcap GitHub repo includes the mcap-protobuf-support Python package to help you write an MCAP reader and writer for Protobuf.","title":"Read and write MCAP"},{"location":"getting-started/protobuf.html#guides","text":"Python C++","title":"Guides"},{"location":"getting-started/protobuf.html#examples","text":"Python - reader and writer C++ - dynamic reader , static reader , and writer","title":"Examples"},{"location":"getting-started/protobuf.html#inspect-mcap","text":"Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo .","title":"Inspect MCAP"},{"location":"getting-started/protobuf.html#visualize-mcap","text":"Foxglove Studio supports playing back local and remote MCAP files containing Protobuf data.","title":"Visualize MCAP"},{"location":"getting-started/ros-1.html","text":"ROS 1 Convert to MCAP To convert your existing ROS 1 bag files into MCAP files, install the mcap CLI tool and run the following command: $ mcap convert ../../testdata/bags/demo.bag demo.mcap You can also use the mcap CLI tool to inspect MCAP files, validate them, and even echo their messages to stdout . For a full list of possible commands, check out the mcap GitHub repo . Read and write MCAP If you\u2019re starting from scratch, you can write code that allows you to write your ROS 1 data to MCAP files and subsequently read your ROS 1 data from your MCAP files . The mcap GitHub repo includes the mcap-ros1-support Python package to help you write an MCAP reader and writer for ROS 1. Guides Python Examples Python - reader and writer Inspect MCAP Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo . Visualize MCAP Foxglove Studio supports playing back local and remote ROS 1 bag files, as well as local and remote MCAP files containing ROS 1 data.","title":"ROS 1"},{"location":"getting-started/ros-1.html#ros-1","text":"","title":"ROS 1"},{"location":"getting-started/ros-1.html#convert-to-mcap","text":"To convert your existing ROS 1 bag files into MCAP files, install the mcap CLI tool and run the following command: $ mcap convert ../../testdata/bags/demo.bag demo.mcap You can also use the mcap CLI tool to inspect MCAP files, validate them, and even echo their messages to stdout . For a full list of possible commands, check out the mcap GitHub repo .","title":"Convert to MCAP"},{"location":"getting-started/ros-1.html#read-and-write-mcap","text":"If you\u2019re starting from scratch, you can write code that allows you to write your ROS 1 data to MCAP files and subsequently read your ROS 1 data from your MCAP files . The mcap GitHub repo includes the mcap-ros1-support Python package to help you write an MCAP reader and writer for ROS 1.","title":"Read and write MCAP"},{"location":"getting-started/ros-1.html#guides","text":"Python","title":"Guides"},{"location":"getting-started/ros-1.html#examples","text":"Python - reader and writer","title":"Examples"},{"location":"getting-started/ros-1.html#inspect-mcap","text":"Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo .","title":"Inspect MCAP"},{"location":"getting-started/ros-1.html#visualize-mcap","text":"Foxglove Studio supports playing back local and remote ROS 1 bag files, as well as local and remote MCAP files containing ROS 1 data.","title":"Visualize MCAP"},{"location":"getting-started/ros-2.html","text":"ROS 2 Convert to MCAP To convert your existing ROS 2 db3 files into MCAP files, install the mcap CLI tool and run the following command: $ mcap convert multiple_files_1.db3 demo.mcap mcap will search the path stored in your $AMENT_PREFIX_PATH environment variable to locate the ROS message definitions on your hard drive. Alternatively, you can specify a colon-separated list of directories for the CLI tool to search using the ament-prefix-path flag: $ mcap convert ros1_input.bag ros1_output.mcap --ament-prefix-path=/your/first/directory;/your/second/directory You can also use the mcap CLI tool to inspect MCAP files, validate them, and even echo their messages to stdout . For a full list of possible commands, check out the mcap GitHub repo . Read and write MCAP If you\u2019re starting from scratch, you can write code that allows you to write your ROS 2 data to MCAP files and subsequently read your ROS 2 data from your MCAP files . Guides Python Examples Python - reader and writer Inspect MCAP Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo . Visualize MCAP Foxglove Studio supports playing back local and remote ROS 2 db3 files, as well as local and remote MCAP files containing ROS 2 data. With that said, we recommend MCAP files over ROS 2 db3 files, as the latter are not completely self-contained.","title":"ROS 2"},{"location":"getting-started/ros-2.html#ros-2","text":"","title":"ROS 2"},{"location":"getting-started/ros-2.html#convert-to-mcap","text":"To convert your existing ROS 2 db3 files into MCAP files, install the mcap CLI tool and run the following command: $ mcap convert multiple_files_1.db3 demo.mcap mcap will search the path stored in your $AMENT_PREFIX_PATH environment variable to locate the ROS message definitions on your hard drive. Alternatively, you can specify a colon-separated list of directories for the CLI tool to search using the ament-prefix-path flag: $ mcap convert ros1_input.bag ros1_output.mcap --ament-prefix-path=/your/first/directory;/your/second/directory You can also use the mcap CLI tool to inspect MCAP files, validate them, and even echo their messages to stdout . For a full list of possible commands, check out the mcap GitHub repo .","title":"Convert to MCAP"},{"location":"getting-started/ros-2.html#read-and-write-mcap","text":"If you\u2019re starting from scratch, you can write code that allows you to write your ROS 2 data to MCAP files and subsequently read your ROS 2 data from your MCAP files .","title":"Read and write MCAP"},{"location":"getting-started/ros-2.html#guides","text":"Python","title":"Guides"},{"location":"getting-started/ros-2.html#examples","text":"Python - reader and writer","title":"Examples"},{"location":"getting-started/ros-2.html#inspect-mcap","text":"Use the mcap CLI tool to inspect MCAP files, validate their contents, and even echo their messages to stdout . For an exhaustive list of ways to interact with your MCAP data, check out the mcap GitHub repo .","title":"Inspect MCAP"},{"location":"getting-started/ros-2.html#visualize-mcap","text":"Foxglove Studio supports playing back local and remote ROS 2 db3 files, as well as local and remote MCAP files containing ROS 2 data. With that said, we recommend MCAP files over ROS 2 db3 files, as the latter are not completely self-contained.","title":"Visualize MCAP"},{"location":"guides/cpp/protobuf.html","text":"Reading and writing Protobuf Info \u201cWriting\u201d section adapted from the original: Recording Robocar Data with MCAP Reading To read Protobuf messages from an MCAP file using C++, we have two options: Static \u2013 Use statically generated class definitions to deserialize the MCAP file Best when there is existing code that uses these Protobuf classes. For example, if you have a simulation that drives a planning module with recorded messages, you already have generated class definitions. It makes sense to take advantage of Protobuf\u2019s existing compatibility mechanisms and use those definitions to deserialize the MCAP data. Dynamic \u2013 Dynamically read fields using the schema definitions in the MCAP file Preferred for inspecting and debugging message content. For example, when building a visualization tool , we want to provide a full view of all fields in a message as it was originally recorded. We can use Protobuf\u2019s DynamicMessage class to enumerate and inspect message fields in this way. Statically generated class definitions First, we generate our class definitions and include the relevant header: #include \"foxglove/PosesInFrame.pb.h\" We also include the MCAP reader implementation: #define MCAP_IMPLEMENTATION #include \"mcap/reader.hpp\" Use the mcap::McapReader::open() method to open an MCAP file for reading: mcap :: McapReader reader ; { const auto res = reader . open ( inputFilename ); if ( ! res . ok ()) { std :: cerr << \"Failed to open \" << inputFilename << \" for reading: \" << res . message << std :: endl ; return 1 ; } } Use a mcap::MessageView to iterate through all of the messages in the MCAP file: auto messageView = reader . readMessages (); for ( auto it = messageView . begin (); it != messageView . end (); it ++ ) { // skip messages that we can't use if (( it -> schema -> encoding != \"protobuf\" ) || it -> schema -> name != \"foxglove.PosesInFrame\" ) { continue ; } foxglove :: PosesInFrame path ; if ( ! path . ParseFromArray ( static_cast < const void *> ( it -> message . data ), it -> message . dataSize )) { std :: cerr << \"could not parse PosesInFrame\" << std :: endl ; return 1 ; } std :: cout << \"Found message: \" << path . ShortDebugString () << std :: endl ; // print out the message } Finally, we close the reader: reader . close (); Dynamically read fields To read message fields dynamically, we must first include the relevant headers: #include <google/protobuf/descriptor.pb.h> #include <google/protobuf/descriptor_database.h> #include <google/protobuf/dynamic_message.h> #define MCAP_IMPLEMENTATION #include \"mcap/reader.hpp\" namespace gp = google :: protobuf ; Then, we construct our mcap::McapReader and mcap::MessageView in the same way as before: mcap :: McapReader reader ; { const auto res = reader . open ( inputFilename ); if ( ! res . ok ()) { std :: cerr << \"Failed to open \" << inputFilename << \" for reading: \" << res . message << std :: endl ; return 1 ; } } auto messageView = reader . readMessages (); Load schema definitions We build a DynamicMessageFactory , using a google::Protobuf::SimpleDescriptorDatabase as the underlying descriptor database. By constructing this ourselves and retaining a reference to the database, we can more easily load that database with definitions from the MCAP file. gp :: SimpleDescriptorDatabase protoDb ; gp :: DescriptorPool protoPool ( & protoDb ); gp :: DynamicMessageFactory protoFactory ( & protoPool ); Now we\u2019re ready to iterate through the messages in the MCAP file. We want to load every message\u2019s FileDescriptorSet into the DescriptorDatabase , if it hasn\u2019t been already: for ( auto it = messageView . begin (); it != messageView . end (); it ++ ) { const gp :: Descriptor * descriptor = protoPool . FindMessageTypeByName ( it -> schema -> name ); if ( descriptor == nullptr ) { if ( ! LoadSchema ( it -> schema , & protoDb )) { reader . close (); return 1 ; } Next, let\u2019s define our LoadSchema() helper function: bool LoadSchema ( const mcap :: SchemaPtr schema , gp :: SimpleDescriptorDatabase * protoDb ) { gp :: FileDescriptorSet fdSet ; if ( ! fdSet . ParseFromArray ( static_cast < const void *> ( schema -> data . data ()), schema -> data . size ())) { std :: cerr << \"failed to parse schema data\" << std :: endl ; return false ; } gp :: FileDescriptorProto unused ; for ( int i = 0 ; i < fdSet . file_size (); ++ i ) { const auto & file = fdSet . file ( i ); if ( ! protoDb -> FindFileByName ( file . name (), & unused )) { if ( ! protoDb -> Add ( file )) { std :: cerr << \"failed to add definition \" << file . name () << \"to protoDB\" << std :: endl ; return false ; } } } return true ; } Print messages Once the FileDescriptorSet is loaded, we can get the descriptor by name: descriptor = protoPool . FindMessageTypeByName ( it -> schema -> name ); We can use this descriptor to parse our message: gp :: Message * message = protoFactory . GetPrototype ( descriptor ) -> New (); if ( ! message -> ParseFromArray ( static_cast < const void *> ( it -> message . data ), it -> message . dataSize )) { std :: cerr << \"failed to parse message using included schema\" << std :: endl ; reader . close (); return 1 ; } std :: cout << message -> ShortDebugString () << std :: endl ; Finally, we close the reader: reader . close (); Writing Create an MCAP writer to start writing your Protobuf messages: mcap :: McapWriter writer ; mcap :: McapWriterOptions opts ( \"protobuf\" ); auto s = writer . open ( \"output.mcap\" ); if ( ! s . ok ) { std :: cerr << \"Failed to open mcap writer: \" << status . message << \" \\n \" ; throw std :: runtime_error ( \"could not open mcap writer\" ); } Configure it to your desired specifications using McapWriterOptions . For example, opts.compressionLevel = mcap::CompressionLevel::Fast will customize your writer to use a faster compression level. Register schema Before we can write messages, we need to register a schema. You must use the fully-qualified name of the message type (e.g. foxglove.PosesInFrame ) and provide a serialized google::protobuf::FileDescriptorSet for the schema itself. Generated Protobuf messages will contain enough information to reconstruct this FileDescriptorSet schema at runtime: // Recursively adds all `fd` dependencies to `fd_set`. void fdSetInternal ( google :: protobuf :: FileDescriptorSet & fd_set , std :: unordered_set < std :: string >& files , const google :: protobuf :: FileDescriptor * fd ) { for ( int i = 0 ; i < fd -> dependency_count (); ++ i ) { const auto * dep = fd -> dependency ( i ); auto [ _ , inserted ] = files . insert ( dep -> name ()); if ( ! inserted ) continue ; fdSetInternal ( fd_set , files , fd -> dependency ( i )); } fd -> CopyTo ( fd_set . add_file ()); } // Returns a serialized google::protobuf::FileDescriptorSet containing // the necessary google::protobuf::FileDescriptor's to describe d. std :: string fdSet ( const google :: protobuf :: Descriptor * d ) { std :: string res ; std :: unordered_set < std :: string > files ; google :: protobuf :: FileDescriptorSet fd_set ; fdSetInternal ( fd_set , files , d -> file ()); return fd_set . SerializeAsString (); } mcap :: Schema createSchema ( const google :: protobuf :: Descriptor * d ) { mcap :: Schema schema ( d -> full_name (), \"protobuf\" , fdSet ( d )); return schema ; } // Create a schema for the foxglove.PosesInFrame message. mcap :: Schema path_schema = createSchema ( foxglove :: PosesInFrame :: descriptor ()); writer . addSchema ( path_schema ); // Assigned schema id is written to path_schema.id Register channel Next, we\u2019ll register a channel to write our messages to: mcap :: Channel path_channel ( \"/planner/path\" , \"protobuf\" , path_schema . id ); mcap . addChannel ( path_channel ); // Assigned channel id written to path_channel.id Write messages We can now finally write messages to the channel using its ID: foxglove :: PosesInFrame poses_msg ; // Fill in path_msg. uint64_t timestamp_ns = std :: chrono :: duration_cast < std :: chrono :: nanoseconds > ( std :: chrono :: system_clock :: now (). time_since_epoch ()) . count (); poses_msg . mutable_timestamp () -> set_seconds ( timestamp_ns / 1'000'000'000ull ) poses_msg . mutable_timestamp () -> set_nanos ( timestamp_ns % 1'000'000'000ull ) poses_msg . set_frame_id ( \"base_link\" ) // Example path in a straight line down the X axis for ( int i = 0 ; i < 10 ; ++ i ) { auto pose = poses_msg . add_poses (); pose -> mutable_position () -> set_x ( i ); pose -> mutable_position () -> set_y ( 0 ); pose -> mutable_position () -> set_z ( 0 ); pose -> mutable_orientation () -> set_x ( 0 ); pose -> mutable_orientation () -> set_y ( 0 ); pose -> mutable_orientation () -> set_z ( 0 ); pose -> mutable_orientation () -> set_w ( 1 ); } std :: string data = poses_msg . SerializeAsString (); mcap :: Message msg ; msg . channelId = path_channel . id ; msg . logTime = timestamp_ns ; msg . publishTime = msg . logTime ; msg . data = reinterpret_cast < const std :: byte *> ( data . data ()); msg . dataSize = data . size (); writer . write ( msg ); Don\u2019t forget to close the writer when you\u2019re done: writer . close (); Inspect MCAP file Now, we can inspect our output MCAP file\u2019s messages. Use the Data source dialog in Foxglove Studio to \u201cOpen local file\u201d. Add a few relevant panels ( Plot , Image , Raw Messages , 3D ) to visualize the robot\u2019s performance. Important links Example code Foxglove schemas","title":"Reading and writing Protobuf"},{"location":"guides/cpp/protobuf.html#reading-and-writing-protobuf","text":"Info \u201cWriting\u201d section adapted from the original: Recording Robocar Data with MCAP","title":"Reading and writing Protobuf"},{"location":"guides/cpp/protobuf.html#reading","text":"To read Protobuf messages from an MCAP file using C++, we have two options: Static \u2013 Use statically generated class definitions to deserialize the MCAP file Best when there is existing code that uses these Protobuf classes. For example, if you have a simulation that drives a planning module with recorded messages, you already have generated class definitions. It makes sense to take advantage of Protobuf\u2019s existing compatibility mechanisms and use those definitions to deserialize the MCAP data. Dynamic \u2013 Dynamically read fields using the schema definitions in the MCAP file Preferred for inspecting and debugging message content. For example, when building a visualization tool , we want to provide a full view of all fields in a message as it was originally recorded. We can use Protobuf\u2019s DynamicMessage class to enumerate and inspect message fields in this way.","title":"Reading"},{"location":"guides/cpp/protobuf.html#statically-generated-class-definitions","text":"First, we generate our class definitions and include the relevant header: #include \"foxglove/PosesInFrame.pb.h\" We also include the MCAP reader implementation: #define MCAP_IMPLEMENTATION #include \"mcap/reader.hpp\" Use the mcap::McapReader::open() method to open an MCAP file for reading: mcap :: McapReader reader ; { const auto res = reader . open ( inputFilename ); if ( ! res . ok ()) { std :: cerr << \"Failed to open \" << inputFilename << \" for reading: \" << res . message << std :: endl ; return 1 ; } } Use a mcap::MessageView to iterate through all of the messages in the MCAP file: auto messageView = reader . readMessages (); for ( auto it = messageView . begin (); it != messageView . end (); it ++ ) { // skip messages that we can't use if (( it -> schema -> encoding != \"protobuf\" ) || it -> schema -> name != \"foxglove.PosesInFrame\" ) { continue ; } foxglove :: PosesInFrame path ; if ( ! path . ParseFromArray ( static_cast < const void *> ( it -> message . data ), it -> message . dataSize )) { std :: cerr << \"could not parse PosesInFrame\" << std :: endl ; return 1 ; } std :: cout << \"Found message: \" << path . ShortDebugString () << std :: endl ; // print out the message } Finally, we close the reader: reader . close ();","title":"Statically generated class definitions"},{"location":"guides/cpp/protobuf.html#dynamically-read-fields","text":"To read message fields dynamically, we must first include the relevant headers: #include <google/protobuf/descriptor.pb.h> #include <google/protobuf/descriptor_database.h> #include <google/protobuf/dynamic_message.h> #define MCAP_IMPLEMENTATION #include \"mcap/reader.hpp\" namespace gp = google :: protobuf ; Then, we construct our mcap::McapReader and mcap::MessageView in the same way as before: mcap :: McapReader reader ; { const auto res = reader . open ( inputFilename ); if ( ! res . ok ()) { std :: cerr << \"Failed to open \" << inputFilename << \" for reading: \" << res . message << std :: endl ; return 1 ; } } auto messageView = reader . readMessages ();","title":"Dynamically read fields"},{"location":"guides/cpp/protobuf.html#load-schema-definitions","text":"We build a DynamicMessageFactory , using a google::Protobuf::SimpleDescriptorDatabase as the underlying descriptor database. By constructing this ourselves and retaining a reference to the database, we can more easily load that database with definitions from the MCAP file. gp :: SimpleDescriptorDatabase protoDb ; gp :: DescriptorPool protoPool ( & protoDb ); gp :: DynamicMessageFactory protoFactory ( & protoPool ); Now we\u2019re ready to iterate through the messages in the MCAP file. We want to load every message\u2019s FileDescriptorSet into the DescriptorDatabase , if it hasn\u2019t been already: for ( auto it = messageView . begin (); it != messageView . end (); it ++ ) { const gp :: Descriptor * descriptor = protoPool . FindMessageTypeByName ( it -> schema -> name ); if ( descriptor == nullptr ) { if ( ! LoadSchema ( it -> schema , & protoDb )) { reader . close (); return 1 ; } Next, let\u2019s define our LoadSchema() helper function: bool LoadSchema ( const mcap :: SchemaPtr schema , gp :: SimpleDescriptorDatabase * protoDb ) { gp :: FileDescriptorSet fdSet ; if ( ! fdSet . ParseFromArray ( static_cast < const void *> ( schema -> data . data ()), schema -> data . size ())) { std :: cerr << \"failed to parse schema data\" << std :: endl ; return false ; } gp :: FileDescriptorProto unused ; for ( int i = 0 ; i < fdSet . file_size (); ++ i ) { const auto & file = fdSet . file ( i ); if ( ! protoDb -> FindFileByName ( file . name (), & unused )) { if ( ! protoDb -> Add ( file )) { std :: cerr << \"failed to add definition \" << file . name () << \"to protoDB\" << std :: endl ; return false ; } } } return true ; }","title":"Load schema definitions"},{"location":"guides/cpp/protobuf.html#print-messages","text":"Once the FileDescriptorSet is loaded, we can get the descriptor by name: descriptor = protoPool . FindMessageTypeByName ( it -> schema -> name ); We can use this descriptor to parse our message: gp :: Message * message = protoFactory . GetPrototype ( descriptor ) -> New (); if ( ! message -> ParseFromArray ( static_cast < const void *> ( it -> message . data ), it -> message . dataSize )) { std :: cerr << \"failed to parse message using included schema\" << std :: endl ; reader . close (); return 1 ; } std :: cout << message -> ShortDebugString () << std :: endl ; Finally, we close the reader: reader . close ();","title":"Print messages"},{"location":"guides/cpp/protobuf.html#writing","text":"Create an MCAP writer to start writing your Protobuf messages: mcap :: McapWriter writer ; mcap :: McapWriterOptions opts ( \"protobuf\" ); auto s = writer . open ( \"output.mcap\" ); if ( ! s . ok ) { std :: cerr << \"Failed to open mcap writer: \" << status . message << \" \\n \" ; throw std :: runtime_error ( \"could not open mcap writer\" ); } Configure it to your desired specifications using McapWriterOptions . For example, opts.compressionLevel = mcap::CompressionLevel::Fast will customize your writer to use a faster compression level.","title":"Writing"},{"location":"guides/cpp/protobuf.html#register-schema","text":"Before we can write messages, we need to register a schema. You must use the fully-qualified name of the message type (e.g. foxglove.PosesInFrame ) and provide a serialized google::protobuf::FileDescriptorSet for the schema itself. Generated Protobuf messages will contain enough information to reconstruct this FileDescriptorSet schema at runtime: // Recursively adds all `fd` dependencies to `fd_set`. void fdSetInternal ( google :: protobuf :: FileDescriptorSet & fd_set , std :: unordered_set < std :: string >& files , const google :: protobuf :: FileDescriptor * fd ) { for ( int i = 0 ; i < fd -> dependency_count (); ++ i ) { const auto * dep = fd -> dependency ( i ); auto [ _ , inserted ] = files . insert ( dep -> name ()); if ( ! inserted ) continue ; fdSetInternal ( fd_set , files , fd -> dependency ( i )); } fd -> CopyTo ( fd_set . add_file ()); } // Returns a serialized google::protobuf::FileDescriptorSet containing // the necessary google::protobuf::FileDescriptor's to describe d. std :: string fdSet ( const google :: protobuf :: Descriptor * d ) { std :: string res ; std :: unordered_set < std :: string > files ; google :: protobuf :: FileDescriptorSet fd_set ; fdSetInternal ( fd_set , files , d -> file ()); return fd_set . SerializeAsString (); } mcap :: Schema createSchema ( const google :: protobuf :: Descriptor * d ) { mcap :: Schema schema ( d -> full_name (), \"protobuf\" , fdSet ( d )); return schema ; } // Create a schema for the foxglove.PosesInFrame message. mcap :: Schema path_schema = createSchema ( foxglove :: PosesInFrame :: descriptor ()); writer . addSchema ( path_schema ); // Assigned schema id is written to path_schema.id","title":"Register schema"},{"location":"guides/cpp/protobuf.html#register-channel","text":"Next, we\u2019ll register a channel to write our messages to: mcap :: Channel path_channel ( \"/planner/path\" , \"protobuf\" , path_schema . id ); mcap . addChannel ( path_channel ); // Assigned channel id written to path_channel.id","title":"Register channel"},{"location":"guides/cpp/protobuf.html#write-messages","text":"We can now finally write messages to the channel using its ID: foxglove :: PosesInFrame poses_msg ; // Fill in path_msg. uint64_t timestamp_ns = std :: chrono :: duration_cast < std :: chrono :: nanoseconds > ( std :: chrono :: system_clock :: now (). time_since_epoch ()) . count (); poses_msg . mutable_timestamp () -> set_seconds ( timestamp_ns / 1'000'000'000ull ) poses_msg . mutable_timestamp () -> set_nanos ( timestamp_ns % 1'000'000'000ull ) poses_msg . set_frame_id ( \"base_link\" ) // Example path in a straight line down the X axis for ( int i = 0 ; i < 10 ; ++ i ) { auto pose = poses_msg . add_poses (); pose -> mutable_position () -> set_x ( i ); pose -> mutable_position () -> set_y ( 0 ); pose -> mutable_position () -> set_z ( 0 ); pose -> mutable_orientation () -> set_x ( 0 ); pose -> mutable_orientation () -> set_y ( 0 ); pose -> mutable_orientation () -> set_z ( 0 ); pose -> mutable_orientation () -> set_w ( 1 ); } std :: string data = poses_msg . SerializeAsString (); mcap :: Message msg ; msg . channelId = path_channel . id ; msg . logTime = timestamp_ns ; msg . publishTime = msg . logTime ; msg . data = reinterpret_cast < const std :: byte *> ( data . data ()); msg . dataSize = data . size (); writer . write ( msg ); Don\u2019t forget to close the writer when you\u2019re done: writer . close ();","title":"Write messages"},{"location":"guides/cpp/protobuf.html#inspect-mcap-file","text":"Now, we can inspect our output MCAP file\u2019s messages. Use the Data source dialog in Foxglove Studio to \u201cOpen local file\u201d. Add a few relevant panels ( Plot , Image , Raw Messages , 3D ) to visualize the robot\u2019s performance.","title":"Inspect MCAP file"},{"location":"guides/cpp/protobuf.html#important-links","text":"Example code Foxglove schemas","title":"Important links"},{"location":"guides/python/json.html","text":"Writing JSON Info From Recording JSON Data to MCAP Files . While JSON might not be the most efficient format for storing point cloud data, it\u2019s easy to get started with as an MCAP beginner. Writing Let\u2019s work with some publicly available data \u2013 the \u201c Sydney Urban Objects Dataset \u201d, released by the Australian Centre for Field Robotics \u2013 and write it as JSON to an MCAP file. This CSV dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR. Each of the 600+ scanned object contains the following fields: t - Timestamp intensity - Laser return intensity id - Laser ID x,y,z - 3D point coordinates azimuth - Horizontal azimuth angle range - Range of laser return pid - Point ID of the original scan Decode CSV data Use Python\u2019s built-in csv and datetime libraries to decode this CSV data: def point_reader ( csv_path : typing . Union [ str , Path ]): with open ( csv_path , \"r\" ) as f : for timestring , i , _ , x , y , z , _ , _ , _ in csv . reader ( f ): timestamp = datetime . datetime . strptime ( timestring , \"%Y%m %d T%H%M%S. %f \" ) yield ( timestamp , float ( i ), float ( x ), float ( y ), float ( z )) This prints out the timestamp, intensity, and coordinates for each point in the CSV file you choose to read in. Encode data as a foxglove.PointCloud Let\u2019s encode this CSV data as a foxglove.PointCloud schema , so we can later visualize this data in Foxglove Studio : field type description timestamp time Timestamp of point cloud frame_id string Frame of reference pose Pose The origin of the point cloud relative to the frame of reference point_stride uint32 Number of bytes between points in the data fields PackedElementField[] Fields in the data data bytes Point data, interpreted using fields The foxglove.PointCloud schema expects a data field that contains a single base64-encoded buffer with all point data, as well as a fields field that contains metadata describing how to decode the data . Since foxglove.PointCloud requires a single timestamp, let\u2019s get it from the first point we see in our file. Then, we\u2019ll pack each field as a four byte single-precision little-endian float. Let\u2019s start by describing the layout of our data in a foxglove.PointCloud message: float32 = 7 # as defined in the schema pointcloud = { \"point_stride\" : ( 4 + 4 + 4 + 4 ), # four bytes per float \"fields\" : [ { \"name\" : \"x\" , \"offset\" : 0 , \"type\" : float32 }, { \"name\" : \"y\" , \"offset\" : 4 , \"type\" : float32 }, { \"name\" : \"z\" , \"offset\" : 8 , \"type\" : float32 }, { \"name\" : \"i\" , \"offset\" : 12 , \"type\" : float32 }, ], } Next, let\u2019s pack the points using Python\u2019s built-in struct and base64 libraries. points = bytearray () base_timestamp = None for point_timestamp , intensity , x , y , z in point_reader ( args . csv ): if base_timestamp is None : base_timestamp = point_timestamp points . extend ( struct . pack ( \"&lt;ffff\" , x , y , z , intensity )) assert base_timestamp is not None , \"found no points in input csv\" pointcloud [ \"data\" ] = base64 . b64encode ( points ) . decode ( \"utf-8\" ) Each 3D object must exist in its own coordinate frame. A point cloud\u2019s frame_id identifies the coordinate frame it belongs in, and its pose determines its relative position from that coordinate frame\u2019s center. Since we will only have one coordinate frame in our MCAP file, we can choose any arbitrary string as our frame_id , and use the identity pose to place our point cloud in its center. pointcloud [ \"pose\" ] = { \"position\" : { \"x\" : 0 , \"y\" : 0 , \"z\" : 0 }, \"orientation\" : { \"x\" : 0 , \"y\" : 0 , \"z\" : 0 , \"w\" : 1 }, } pointcloud [ \"frame_id\" ] = \"lidar\" We\u2019ll leave the timestamp field for later, when we write the messages into the MCAP file. Create a writer We\u2019ll start with some imports from the Python MCAP library: from mcap.mcap0.writer import Writer from mcap.mcap0.well_known import SchemaEncoding , MessageEncoding Open a file where we\u2019ll output our MCAP data and write our header: with open ( args . output , \"wb\" ) as f : writer = Writer ( f ) writer . start () Register channel Let\u2019s create a channel of messages to contain our point cloud. The schema\u2019s name and content tell Foxglove Studio that it can parse and display this message as a point cloud. with open ( Path ( __file__ ) . parent / \"PointCloud.json\" , \"rb\" ) as f : schema = f . read () schema_id = writer . register_schema ( name = \"foxglove.PointCloud\" , encoding = SchemaEncoding . JSONSchema , data = schema , ) channel_id = writer . register_channel ( topic = \"pointcloud\" , message_encoding = MessageEncoding . JSON , schema_id = schema_id , ) Write messages Let\u2019s write a single foxglove.PointCloud message on the channel we just created: pointcloud [ \"timestamp\" ] = { \"sec\" : int ( base_timestamp . timestamp ()), \"nsec\" : base_timestamp . microsecond * 1000 , } writer . add_message ( channel_id , log_time = int ( base_timestamp . timestamp () * 1e9 ), data = json . dumps ( pointcloud ) . encode ( \"utf-8\" ), publish_time = int ( base_timestamp . timestamp () * 1e9 ), ) Close the MCAP writer to include the summary and footer in your output MCAP file: writer . finish () That\u2019s it! We now have a valid MCAP file with a single point cloud message. Inspect your file To inspect your MCAP file, install the MCAP CLI tool . Run the following commands to summarize your file\u2019s contents and to verify that it has no issues: $ mcap info output.mcap library: my-excellent-library profile: messages: 1 duration: 0s start: 2011 -11-04T01:36:05.987339008+11:00 ( 1320330965 .987339008 ) end: 2011 -11-04T01:36:05.987339008+11:00 ( 1320330965 .987339008 ) compression: zstd: [ 1 /1 chunks ] ( 48 .09% ) channels: ( 1 ) pointcloud 1 msgs ( +Inf Hz ) : foxglove.PointCloud [ jsonschema ] attachments: 0 $ mcap doctor output.mcap Examining output.mcap Visualize data For a more visual representation of this data, let\u2019s use Foxglove Studio. Open either the desktop or web app , and add a Raw Messages and our newly added 3D (Beta) panel to your layout. Then, simply drag and drop your output MCAP file into the app window to start playing the data. Make sure to enable the pointcloud topic in the 3D (Beta) panel to display the point cloud in 3D space. You can also inspect the raw data for the pointcloud topic in your Raw Messages panel: Important links MCAP Python library Example code","title":"Writing JSON"},{"location":"guides/python/json.html#writing-json","text":"Info From Recording JSON Data to MCAP Files . While JSON might not be the most efficient format for storing point cloud data, it\u2019s easy to get started with as an MCAP beginner.","title":"Writing JSON"},{"location":"guides/python/json.html#writing","text":"Let\u2019s work with some publicly available data \u2013 the \u201c Sydney Urban Objects Dataset \u201d, released by the Australian Centre for Field Robotics \u2013 and write it as JSON to an MCAP file. This CSV dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR. Each of the 600+ scanned object contains the following fields: t - Timestamp intensity - Laser return intensity id - Laser ID x,y,z - 3D point coordinates azimuth - Horizontal azimuth angle range - Range of laser return pid - Point ID of the original scan","title":"Writing"},{"location":"guides/python/json.html#decode-csv-data","text":"Use Python\u2019s built-in csv and datetime libraries to decode this CSV data: def point_reader ( csv_path : typing . Union [ str , Path ]): with open ( csv_path , \"r\" ) as f : for timestring , i , _ , x , y , z , _ , _ , _ in csv . reader ( f ): timestamp = datetime . datetime . strptime ( timestring , \"%Y%m %d T%H%M%S. %f \" ) yield ( timestamp , float ( i ), float ( x ), float ( y ), float ( z )) This prints out the timestamp, intensity, and coordinates for each point in the CSV file you choose to read in.","title":"Decode CSV data"},{"location":"guides/python/json.html#encode-data-as-a-foxglovepointcloud","text":"Let\u2019s encode this CSV data as a foxglove.PointCloud schema , so we can later visualize this data in Foxglove Studio : field type description timestamp time Timestamp of point cloud frame_id string Frame of reference pose Pose The origin of the point cloud relative to the frame of reference point_stride uint32 Number of bytes between points in the data fields PackedElementField[] Fields in the data data bytes Point data, interpreted using fields The foxglove.PointCloud schema expects a data field that contains a single base64-encoded buffer with all point data, as well as a fields field that contains metadata describing how to decode the data . Since foxglove.PointCloud requires a single timestamp, let\u2019s get it from the first point we see in our file. Then, we\u2019ll pack each field as a four byte single-precision little-endian float. Let\u2019s start by describing the layout of our data in a foxglove.PointCloud message: float32 = 7 # as defined in the schema pointcloud = { \"point_stride\" : ( 4 + 4 + 4 + 4 ), # four bytes per float \"fields\" : [ { \"name\" : \"x\" , \"offset\" : 0 , \"type\" : float32 }, { \"name\" : \"y\" , \"offset\" : 4 , \"type\" : float32 }, { \"name\" : \"z\" , \"offset\" : 8 , \"type\" : float32 }, { \"name\" : \"i\" , \"offset\" : 12 , \"type\" : float32 }, ], } Next, let\u2019s pack the points using Python\u2019s built-in struct and base64 libraries. points = bytearray () base_timestamp = None for point_timestamp , intensity , x , y , z in point_reader ( args . csv ): if base_timestamp is None : base_timestamp = point_timestamp points . extend ( struct . pack ( \"&lt;ffff\" , x , y , z , intensity )) assert base_timestamp is not None , \"found no points in input csv\" pointcloud [ \"data\" ] = base64 . b64encode ( points ) . decode ( \"utf-8\" ) Each 3D object must exist in its own coordinate frame. A point cloud\u2019s frame_id identifies the coordinate frame it belongs in, and its pose determines its relative position from that coordinate frame\u2019s center. Since we will only have one coordinate frame in our MCAP file, we can choose any arbitrary string as our frame_id , and use the identity pose to place our point cloud in its center. pointcloud [ \"pose\" ] = { \"position\" : { \"x\" : 0 , \"y\" : 0 , \"z\" : 0 }, \"orientation\" : { \"x\" : 0 , \"y\" : 0 , \"z\" : 0 , \"w\" : 1 }, } pointcloud [ \"frame_id\" ] = \"lidar\" We\u2019ll leave the timestamp field for later, when we write the messages into the MCAP file.","title":"Encode data as a foxglove.PointCloud"},{"location":"guides/python/json.html#create-a-writer","text":"We\u2019ll start with some imports from the Python MCAP library: from mcap.mcap0.writer import Writer from mcap.mcap0.well_known import SchemaEncoding , MessageEncoding Open a file where we\u2019ll output our MCAP data and write our header: with open ( args . output , \"wb\" ) as f : writer = Writer ( f ) writer . start ()","title":"Create a writer"},{"location":"guides/python/json.html#register-channel","text":"Let\u2019s create a channel of messages to contain our point cloud. The schema\u2019s name and content tell Foxglove Studio that it can parse and display this message as a point cloud. with open ( Path ( __file__ ) . parent / \"PointCloud.json\" , \"rb\" ) as f : schema = f . read () schema_id = writer . register_schema ( name = \"foxglove.PointCloud\" , encoding = SchemaEncoding . JSONSchema , data = schema , ) channel_id = writer . register_channel ( topic = \"pointcloud\" , message_encoding = MessageEncoding . JSON , schema_id = schema_id , )","title":"Register channel"},{"location":"guides/python/json.html#write-messages","text":"Let\u2019s write a single foxglove.PointCloud message on the channel we just created: pointcloud [ \"timestamp\" ] = { \"sec\" : int ( base_timestamp . timestamp ()), \"nsec\" : base_timestamp . microsecond * 1000 , } writer . add_message ( channel_id , log_time = int ( base_timestamp . timestamp () * 1e9 ), data = json . dumps ( pointcloud ) . encode ( \"utf-8\" ), publish_time = int ( base_timestamp . timestamp () * 1e9 ), ) Close the MCAP writer to include the summary and footer in your output MCAP file: writer . finish () That\u2019s it! We now have a valid MCAP file with a single point cloud message.","title":"Write messages"},{"location":"guides/python/json.html#inspect-your-file","text":"To inspect your MCAP file, install the MCAP CLI tool . Run the following commands to summarize your file\u2019s contents and to verify that it has no issues: $ mcap info output.mcap library: my-excellent-library profile: messages: 1 duration: 0s start: 2011 -11-04T01:36:05.987339008+11:00 ( 1320330965 .987339008 ) end: 2011 -11-04T01:36:05.987339008+11:00 ( 1320330965 .987339008 ) compression: zstd: [ 1 /1 chunks ] ( 48 .09% ) channels: ( 1 ) pointcloud 1 msgs ( +Inf Hz ) : foxglove.PointCloud [ jsonschema ] attachments: 0 $ mcap doctor output.mcap Examining output.mcap","title":"Inspect your file"},{"location":"guides/python/json.html#visualize-data","text":"For a more visual representation of this data, let\u2019s use Foxglove Studio. Open either the desktop or web app , and add a Raw Messages and our newly added 3D (Beta) panel to your layout. Then, simply drag and drop your output MCAP file into the app window to start playing the data. Make sure to enable the pointcloud topic in the 3D (Beta) panel to display the point cloud in 3D space. You can also inspect the raw data for the pointcloud topic in your Raw Messages panel:","title":"Visualize data"},{"location":"guides/python/json.html#important-links","text":"MCAP Python library Example code","title":"Important links"},{"location":"guides/python/protobuf.html","text":"Reading and writing Protobuf To start writing Python code that reads and writes Protobuf data in MCAP, install the mcap-protobuf-support helper library . Reading To read in Protobuf data from an MCAP file ( my_data.mcap ), use the high-level reader interface. import sys from mcap_protobuf.reader import read_protobuf_messages def main (): for msg in read_protobuf_messages ( \"my_data.mcap\" ): print ( f \" { msg . topic } : { msg . proto_msg } \" ) if __name__ == \"__main__\" : main () Writing import sys from mcap_protobuf.writer import Writer from complex_message_pb2 import ComplexMessage from simple_message_pb2 import SimpleMessage def main (): with open ( sys . argv [ 1 ], \"wb\" ) as f , Writer ( f ) as mcap_writer : for i in range ( 1 , 11 ): mcap_writer . write_message ( topic = \"/simple_messages\" , message = SimpleMessage ( data = f \"Hello MCAP protobuf world # { i } !\" ), log_time = i * 1000 , publish_time = i * 1000 , ) complex_message = ComplexMessage ( fieldA = f \"Field A { i } \" , fieldB = f \"Field B { i } \" ) mcap_writer . write_message ( topic = \"/complex_messages\" , message = complex_message , log_time = i * 1000 , publish_time = i * 1000 , ) if __name__ == \"__main__\" : main () Important links MCAP Python library mcap-protobuf-support helper library Example code","title":"Reading and writing Protobuf"},{"location":"guides/python/protobuf.html#reading-and-writing-protobuf","text":"To start writing Python code that reads and writes Protobuf data in MCAP, install the mcap-protobuf-support helper library .","title":"Reading and writing Protobuf"},{"location":"guides/python/protobuf.html#reading","text":"To read in Protobuf data from an MCAP file ( my_data.mcap ), use the high-level reader interface. import sys from mcap_protobuf.reader import read_protobuf_messages def main (): for msg in read_protobuf_messages ( \"my_data.mcap\" ): print ( f \" { msg . topic } : { msg . proto_msg } \" ) if __name__ == \"__main__\" : main ()","title":"Reading"},{"location":"guides/python/protobuf.html#writing","text":"import sys from mcap_protobuf.writer import Writer from complex_message_pb2 import ComplexMessage from simple_message_pb2 import SimpleMessage def main (): with open ( sys . argv [ 1 ], \"wb\" ) as f , Writer ( f ) as mcap_writer : for i in range ( 1 , 11 ): mcap_writer . write_message ( topic = \"/simple_messages\" , message = SimpleMessage ( data = f \"Hello MCAP protobuf world # { i } !\" ), log_time = i * 1000 , publish_time = i * 1000 , ) complex_message = ComplexMessage ( fieldA = f \"Field A { i } \" , fieldB = f \"Field B { i } \" ) mcap_writer . write_message ( topic = \"/complex_messages\" , message = complex_message , log_time = i * 1000 , publish_time = i * 1000 , ) if __name__ == \"__main__\" : main ()","title":"Writing"},{"location":"guides/python/protobuf.html#important-links","text":"MCAP Python library mcap-protobuf-support helper library Example code","title":"Important links"},{"location":"guides/python/ros1.html","text":"Reading and writing ROS 1 To start writing Python code that reads and writes ROS 1 data in MCAP, install the mcap-ros1-support helper library . Reading Read ROS1 messages from an MCAP file using the mcap_ros1.reader module: import sys from mcap_ros1.reader import read_ros1_messages for msg in read_ros1_messages ( sys . argv [ 1 ]): print ( f \" { msg . topic } : { msg . ros_msg } \" ) Writing Import the necessary packages from Python and a Writer from mcap-ros1-support : import sys from std_msgs.msg import String from mcap_ros1.writer import Writer Open a file and create a writer: with open ( sys . argv [ 1 ], \"wb\" ) as f : ros_writer = Writer ( f ) Finally, specify the channel (\u201c/chatter\u201d) and its schema ( String ) you will be using before publishing your messages (\u201cstring message 0\u201d, \u201cstring message 1\u201d, \u201cstring message 2\u201d, etc.): for i in range ( 0 , 10 ): ros_writer . write_message ( \"/chatter\" , String ( data = f \"string message { i } \" )) ros_writer . finish () Important links MCAP Python library mcap-ros1-support helper library Example code","title":"Reading and writing ROS 1"},{"location":"guides/python/ros1.html#reading-and-writing-ros-1","text":"To start writing Python code that reads and writes ROS 1 data in MCAP, install the mcap-ros1-support helper library .","title":"Reading and writing ROS 1"},{"location":"guides/python/ros1.html#reading","text":"Read ROS1 messages from an MCAP file using the mcap_ros1.reader module: import sys from mcap_ros1.reader import read_ros1_messages for msg in read_ros1_messages ( sys . argv [ 1 ]): print ( f \" { msg . topic } : { msg . ros_msg } \" )","title":"Reading"},{"location":"guides/python/ros1.html#writing","text":"Import the necessary packages from Python and a Writer from mcap-ros1-support : import sys from std_msgs.msg import String from mcap_ros1.writer import Writer Open a file and create a writer: with open ( sys . argv [ 1 ], \"wb\" ) as f : ros_writer = Writer ( f ) Finally, specify the channel (\u201c/chatter\u201d) and its schema ( String ) you will be using before publishing your messages (\u201cstring message 0\u201d, \u201cstring message 1\u201d, \u201cstring message 2\u201d, etc.): for i in range ( 0 , 10 ): ros_writer . write_message ( \"/chatter\" , String ( data = f \"string message { i } \" )) ros_writer . finish ()","title":"Writing"},{"location":"guides/python/ros1.html#important-links","text":"MCAP Python library mcap-ros1-support helper library Example code","title":"Important links"},{"location":"guides/python/ros2.html","text":"Reading ROS 2 To start writing Python code that reads ROS 2 data from MCAP, install the rosbag2_py package . Reading We\u2019ll start with our imports: import argparse from rclpy.serialization import deserialize_message from rosidl_runtime_py.utilities import get_message import rosbag2_py Next, we\u2019ll write a function for reading our ROS 2 messages from an MCAP file: def read_messages ( input_bag : str ): reader = rosbag2_py . SequentialReader () reader . open ( rosbag2_py . StorageOptions ( uri = input_bag , storage_id = \"mcap\" ), rosbag2_py . ConverterOptions ( input_serialization_format = \"cdr\" , output_serialization_format = \"cdr\" ), ) topic_types = reader . get_all_topics_and_types () def typename ( topic_name ): for topic_type in topic_types : if topic_type . name == topic_name : return topic_type . type raise ValueError ( f \"topic { topic_name } not in bag\" ) while reader . has_next (): topic , data , timestamp = reader . read_next () msg_type = get_message ( typename ( topic )) msg = deserialize_message ( data , msg_type ) yield topic , msg , timestamp del reader Finally, we\u2019ll print out each message with its topic, data type, timestamp, and contents: def main (): parser = argparse . ArgumentParser ( description = __doc__ ) parser . add_argument ( \"input\" , help = \"input bag path (folder or filepath) to read from\" ) args = parser . parse_args () for topic , msg , timestamp in read_messages ( args . input ): print ( f \" { topic } ( { type ( msg ) . __name__ } ) [ { timestamp } ]: ' { msg . data } '\" ) if __name__ == \"__main__\" : main () Important links MCAP Python library Example code","title":"Reading ROS 2"},{"location":"guides/python/ros2.html#reading-ros-2","text":"To start writing Python code that reads ROS 2 data from MCAP, install the rosbag2_py package .","title":"Reading ROS 2"},{"location":"guides/python/ros2.html#reading","text":"We\u2019ll start with our imports: import argparse from rclpy.serialization import deserialize_message from rosidl_runtime_py.utilities import get_message import rosbag2_py Next, we\u2019ll write a function for reading our ROS 2 messages from an MCAP file: def read_messages ( input_bag : str ): reader = rosbag2_py . SequentialReader () reader . open ( rosbag2_py . StorageOptions ( uri = input_bag , storage_id = \"mcap\" ), rosbag2_py . ConverterOptions ( input_serialization_format = \"cdr\" , output_serialization_format = \"cdr\" ), ) topic_types = reader . get_all_topics_and_types () def typename ( topic_name ): for topic_type in topic_types : if topic_type . name == topic_name : return topic_type . type raise ValueError ( f \"topic { topic_name } not in bag\" ) while reader . has_next (): topic , data , timestamp = reader . read_next () msg_type = get_message ( typename ( topic )) msg = deserialize_message ( data , msg_type ) yield topic , msg , timestamp del reader Finally, we\u2019ll print out each message with its topic, data type, timestamp, and contents: def main (): parser = argparse . ArgumentParser ( description = __doc__ ) parser . add_argument ( \"input\" , help = \"input bag path (folder or filepath) to read from\" ) args = parser . parse_args () for topic , msg , timestamp in read_messages ( args . input ): print ( f \" { topic } ( { type ( msg ) . __name__ } ) [ { timestamp } ]: ' { msg . data } '\" ) if __name__ == \"__main__\" : main ()","title":"Reading"},{"location":"guides/python/ros2.html#important-links","text":"MCAP Python library Example code","title":"Important links"},{"location":"home/data-serialization-formats.html","text":"Data Serialization Formats MCAP files can store multiple streams of heterogeneous data, serialized in any of the following data formats: ROS 1 ROS 2 JSON Schema Protobuf FlatBuffers To start reading and writing MCAP messages in these formats, check out the Python and C++ tutorials in our Guides section.","title":"Data Serialization Formats"},{"location":"home/data-serialization-formats.html#data-serialization-formats","text":"MCAP files can store multiple streams of heterogeneous data, serialized in any of the following data formats: ROS 1 ROS 2 JSON Schema Protobuf FlatBuffers To start reading and writing MCAP messages in these formats, check out the Python and C++ tutorials in our Guides section.","title":"Data Serialization Formats"},{"location":"home/messages-channels-and-schemas.html","text":"Messages, Channels and Schemas MCAP organizes its data via messages, channels, and schemas. Message The unit of communication between nodes in the pub/sub system. Channel A stream of messages which have the same type, or schema. Often corresponds to a connection between a publisher and a subscriber. Schema A description of the structure and contents of messages on a channel, e.g. a Protobuf FileDescriptorSet or JSON Schema. Info The @foxglove/schemas repo provides pre-defined schema definitions for Foxglove Studio visualizations. Write messages that adhere to these schemas to an MCAP file, then visualize and debug this data using Studio\u2019s panels .","title":"Messages, Channels and Schemas"},{"location":"home/messages-channels-and-schemas.html#messages-channels-and-schemas","text":"MCAP organizes its data via messages, channels, and schemas.","title":"Messages, Channels and Schemas"},{"location":"home/messages-channels-and-schemas.html#message","text":"The unit of communication between nodes in the pub/sub system.","title":"Message"},{"location":"home/messages-channels-and-schemas.html#channel","text":"A stream of messages which have the same type, or schema. Often corresponds to a connection between a publisher and a subscriber.","title":"Channel"},{"location":"home/messages-channels-and-schemas.html#schema","text":"A description of the structure and contents of messages on a channel, e.g. a Protobuf FileDescriptorSet or JSON Schema. Info The @foxglove/schemas repo provides pre-defined schema definitions for Foxglove Studio visualizations. Write messages that adhere to these schemas to an MCAP file, then visualize and debug this data using Studio\u2019s panels .","title":"Schema"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html","text":"Evaluation of robotics data recording file formats Oct 6, 2021 John Hurliman ( @jhurliman ) Foxglove Originally posted on ROS Discourse The transition from ROS 1 to ROS 2 involves switching recording file formats from the classic \u201crosbag\u201d (.bag) file format to a new serialization format. The rosbag2 recording software includes a pluggable interface to support different serialization methods, with the current default using SQLite (.db3) files. Non-ROS robotics frameworks each use their own recording format, leading to further fragmentation. This document reviews desirable properties of a robotics data recording format (including those considered during ROS 2 development) and evaluates existing and proposed solutions against those requirements. Problem Statement The goal of recording robotics data is to store and analyze or replay the data later to understand what environmental inputs were observed, what actions were taken, and some granularity of the internal state to understand what decisions were made and why. This goal is usually accomplished by recording multiple streams of data as packetized messages with high-resolution timestamps. Example data streams include camera images, LIDAR scans, pose estimates, planning decisions, and actuator commands. The streams of data vary significantly in size, frequency, compression, and schema complexity. The following desirable properties have been collected from Karsten Knese\u2019s rosbag2 design proposal and from interviewing robotics companies using a variety of approaches to data recording and data storage. Serialization Agnostic The robotics world has not standardized on a single message serialization format. ROS1 included its own serialization format, while ROS2 uses CDR as the default but includes a pluggable middleware layer to support other serializations. Non-ROS robots may be using Protocol Buffers (Protobuf), CBOR, JSON, or a proprietary format. A generic recording container must include support for different serialization formats to support the breadth of choices for message serialization in use today. Determinism Playing back a recording should be a deterministic process. The ordering of messages in a single stream and across streams must be well defined to enable this. This ordering is generally achieved by associating high-resolution timestamps with each message and iterating across all streams concurrently in timestamp order during playback. There are many different concepts of time in a recording: the time a message was originally published, the time when it was sent by the publisher (this can be very different for messages that are held in a queue and replayed), when the data recording process received it, or when the recorder wrote it to disk. Using a consistent definition of time is the most critical requirement while storing multiple timestamps can enable additional use cases. Self-Describing The design of robotic systems is often evolving. Data streams are added and removed, message publishing frequencies are tuned up and down, and schemas change. For a recording file to be useful as long-term storage, it must be self-describing. That is, the decoding of the container itself must be understandable by looking at a file signature, and the serialization of individual messages must either be self-describing (such as JSON or CBOR) or described by a schema embedded in the recording for each stream. Parsing a recording should not require access to external schemas, a specific codebase revision, or any other external resources. Big and Small Data Some sensor streams can produce gigabytes of data per second. While splitting files along the time axis is always possible, the recording container should support at least hundreds of gigabytes before forcing a file split. The per-message overhead should be low enough to support small messages at 1kHz+ frequency, and large individual messages should be possible to support uncompressed video feeds. Write-Optimized For a file format to be suitable for recording, it must be optimized for write throughput. An append-only file format is desirable for performance, to enable streaming data as it is written, and for crash safety. It should be possible to make I/O and CPU/memory tradeoffs by either compressing data at write time or writing uncompressed data. Things to avoid: expensive index creation, expensive working memory requirements, and expensive transcoding at write time. No A Priori Channel Knowledge Most robotics frameworks use a publish-subscribe (pub/sub) messaging system to send messages over channels, also known as topics. It is not always possible to know the complete list of channels that should be recorded ahead of time or the schemas for each channel before the channel starts publishing messages. For a recorder to support append-only, it must be possible to write new channel information and schemas midway through writing, after writing has already begun for other channels. Corruption Resilient Robots operate in chaotic environments. The most critical recording files are often associated with an unpredictable event such as a collision or system failure where data integrity cannot be guaranteed. Some possible mitigations are splitting the data into smaller chunks, adding chunk-level checksumming, and a documented process on how to recover partial data from a truncated or otherwise corrupted recording. Read-Optimized There are many kinds of read-optimization, and often there is a direct tradeoff between read-optimization and write optimization. A recording format should be optimized for write throughput first and foremost. However, the generated files should be readable without loading the entire file into memory, parsable in different environments (desktop, server, web), and support efficient random access and range requests if possible. Although it is possible to use different file formats for recording and archiving with a transcoding process, if a file format can support high write throughput and speed up these read operations, it simplifies the development ecosystem. Tooling can then converge around a single file format. Standards Compatible If a universal robotics recording file format is adopted, it should be eligible to become a standard. The standard could take the form of an Internet Engineering Task Force RFC or a normalized specification through another standards body. Typical requirements include real-world usage in multiple places, complete documentation, no patent encumbrance, and at least two independent implementations. Existing Robotics Recording Formats We can compare this set of requirements and desirable properties against existing file formats designed as robot recording containers. rosbag1 The original ROS1 .bag file format meets several of our requirements. It is deterministic, self-describing, supports big and small data, somewhat write-optimized (although not append-only), does not require a priori channel knowledge, somewhat corruption resistant (no checksums, but data can be recovered at the chunk level and the index can be rebuilt), read-optimized via an index, and standards compatible. Its major shortcoming is that it is not serialization agnostic. rosbag2 Whereas rosbag1 refers to both a file format definition and two reference implementations (C++ and Python), rosbag2 is a C API with a plugin interface for implementations, plus a default implementation using SQLite as the on-disk format. Since this document is focused on serialization formats, rosbag2 will be defined as the default SQLite implementation. This format is serialization agnostic, deterministic, but not (yet) self-describing, although there is ongoing work to address that. It does not require a priori channel knowledge, is corruption resistant , and is read-optimized except in the web use case. The significant shortcomings are around write-optimization (not append-only, the index is updated per row insert), lack of compression support at write time beyond the individual message level, and requiring the entire file to be read into memory in the web use case (via sql.js ). SQLite also poses challenges to standardization. The Internet Engineering Task Force, W3C, and other standards bodies require any specification to have two independent implementations to graduate to a standard. SQLite has only ever had a single implementation that is cross-compiled to different platforms, which caused the Web SQL Database proposal to reach an impasse . Length-delimited Protobuf From interviewing various robotics companies, the most popular serialization format outside of ROS is to write protobuf messages to disk with a simple length delimiter or short message header. If timestamps are included in every message, this can be deterministic, but the initial .proto schemas must be bundled as well to meet the self-describing criteria. This approach does not require a priori channel knowledge and supports big and small data. It does not provide any explicit corruption resistance, although a truncated final message can sometimes be detected by a serialization failure. It is a write-optimized append-only approach, and compression can be supported by piping the write output through a compressor. Protobuf has been submitted as an Internet-Draft (I-D) standard. The significant shortcomings are that by nature of defining around a single serialization format, it is not serialization agnostic, and that without a well-defined standard, the files produced by different implementations will be incompatible. The lack of read-optimization excludes its use in playback or analysis tools that require random seeking. Apollo Cyber RT Cyber RT includes its own recording file format. However, the documentation of the format itself (opposed to the software APIs surrounding it) is scarce. It is likely similar to Length-delimited Protobuf for the purpose of this discussion. Existing Big Data Formats Outside of robotic recording file formats, we can evaluate common \u201cbig data\u201d containers for suitability to this problem. Avro - Native Types Avro is a row-based storage format that requires each row to follow a single schema. This requires a union type of all possible message schemas that need to be rewritten as new channels are added into a container or each channel to be written to a separate file. Support for different serialization formats requires mapping each serialization format to Avro data types, which may be a lossy process. Avro - Opaque Bytes Another way of using the Avro format is to define the schema as a single field containing an opaque array of bytes, which supports different serialization formats in a common container. This approach meets many of the desired requirements, but it defeats the most commonly desired benefit of using an existing container format: the ability to ingest data into a third-party system and have that system understand the data in a typed way. A recording format built on Avro would still require standardizing how and where to store content types and per-channel information such as schemas. Regardless, Avro containers storing opaque byte messages may be one possible solution. HDF5 HDF5 predates Avro and suffers from similar limitations while also missing support for deeply nested data via schemas. Using HDF5 as strictly a wrapper around opaque byte messages would require a more complex file format for no additional gain. Parquet Parquet is a columnar data store that is not suitable for random access to messages and is not optimized for replay. While Parquet can play a valuable role in robotics for analysis and long-term storage, its lack of suitability for real-time recording and message playback excludes it from our search for a recording file format. Next Steps By comparing existing robotics recording formats with generic big data container formats, we can see there are practical advantages to designing a recording format specific to the robotics use case. However, none of the existing recording formats meet all the requirements and desirable traits laid out in this document. Many of the proposed approaches (rosbag2 SQLite, Avro with opaque buffer) follow an anti-pattern of using an existing schema-based format but only storing opaque binary blobs, negating the promise of interoperability from using an existing format. In light of this evaluation, I would like to propose a file format tailored to the requirements of robotics recording. I will introduce this file format in a follow-up proposal. The goal is to continue collecting requirements and desirable traits and collaboratively craft an industry standard backed by multiple implementations.","title":"Evaluation of robotics data recording file formats"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#evaluation-of-robotics-data-recording-file-formats","text":"Oct 6, 2021 John Hurliman ( @jhurliman ) Foxglove Originally posted on ROS Discourse The transition from ROS 1 to ROS 2 involves switching recording file formats from the classic \u201crosbag\u201d (.bag) file format to a new serialization format. The rosbag2 recording software includes a pluggable interface to support different serialization methods, with the current default using SQLite (.db3) files. Non-ROS robotics frameworks each use their own recording format, leading to further fragmentation. This document reviews desirable properties of a robotics data recording format (including those considered during ROS 2 development) and evaluates existing and proposed solutions against those requirements.","title":"Evaluation of robotics data recording file formats"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#problem-statement","text":"The goal of recording robotics data is to store and analyze or replay the data later to understand what environmental inputs were observed, what actions were taken, and some granularity of the internal state to understand what decisions were made and why. This goal is usually accomplished by recording multiple streams of data as packetized messages with high-resolution timestamps. Example data streams include camera images, LIDAR scans, pose estimates, planning decisions, and actuator commands. The streams of data vary significantly in size, frequency, compression, and schema complexity. The following desirable properties have been collected from Karsten Knese\u2019s rosbag2 design proposal and from interviewing robotics companies using a variety of approaches to data recording and data storage.","title":"Problem Statement"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#serialization-agnostic","text":"The robotics world has not standardized on a single message serialization format. ROS1 included its own serialization format, while ROS2 uses CDR as the default but includes a pluggable middleware layer to support other serializations. Non-ROS robots may be using Protocol Buffers (Protobuf), CBOR, JSON, or a proprietary format. A generic recording container must include support for different serialization formats to support the breadth of choices for message serialization in use today.","title":"Serialization Agnostic"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#determinism","text":"Playing back a recording should be a deterministic process. The ordering of messages in a single stream and across streams must be well defined to enable this. This ordering is generally achieved by associating high-resolution timestamps with each message and iterating across all streams concurrently in timestamp order during playback. There are many different concepts of time in a recording: the time a message was originally published, the time when it was sent by the publisher (this can be very different for messages that are held in a queue and replayed), when the data recording process received it, or when the recorder wrote it to disk. Using a consistent definition of time is the most critical requirement while storing multiple timestamps can enable additional use cases.","title":"Determinism"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#self-describing","text":"The design of robotic systems is often evolving. Data streams are added and removed, message publishing frequencies are tuned up and down, and schemas change. For a recording file to be useful as long-term storage, it must be self-describing. That is, the decoding of the container itself must be understandable by looking at a file signature, and the serialization of individual messages must either be self-describing (such as JSON or CBOR) or described by a schema embedded in the recording for each stream. Parsing a recording should not require access to external schemas, a specific codebase revision, or any other external resources.","title":"Self-Describing"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#big-and-small-data","text":"Some sensor streams can produce gigabytes of data per second. While splitting files along the time axis is always possible, the recording container should support at least hundreds of gigabytes before forcing a file split. The per-message overhead should be low enough to support small messages at 1kHz+ frequency, and large individual messages should be possible to support uncompressed video feeds.","title":"Big and Small Data"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#write-optimized","text":"For a file format to be suitable for recording, it must be optimized for write throughput. An append-only file format is desirable for performance, to enable streaming data as it is written, and for crash safety. It should be possible to make I/O and CPU/memory tradeoffs by either compressing data at write time or writing uncompressed data. Things to avoid: expensive index creation, expensive working memory requirements, and expensive transcoding at write time.","title":"Write-Optimized"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#no-a-priori-channel-knowledge","text":"Most robotics frameworks use a publish-subscribe (pub/sub) messaging system to send messages over channels, also known as topics. It is not always possible to know the complete list of channels that should be recorded ahead of time or the schemas for each channel before the channel starts publishing messages. For a recorder to support append-only, it must be possible to write new channel information and schemas midway through writing, after writing has already begun for other channels.","title":"No A Priori Channel Knowledge"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#corruption-resilient","text":"Robots operate in chaotic environments. The most critical recording files are often associated with an unpredictable event such as a collision or system failure where data integrity cannot be guaranteed. Some possible mitigations are splitting the data into smaller chunks, adding chunk-level checksumming, and a documented process on how to recover partial data from a truncated or otherwise corrupted recording.","title":"Corruption Resilient"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#read-optimized","text":"There are many kinds of read-optimization, and often there is a direct tradeoff between read-optimization and write optimization. A recording format should be optimized for write throughput first and foremost. However, the generated files should be readable without loading the entire file into memory, parsable in different environments (desktop, server, web), and support efficient random access and range requests if possible. Although it is possible to use different file formats for recording and archiving with a transcoding process, if a file format can support high write throughput and speed up these read operations, it simplifies the development ecosystem. Tooling can then converge around a single file format.","title":"Read-Optimized"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#standards-compatible","text":"If a universal robotics recording file format is adopted, it should be eligible to become a standard. The standard could take the form of an Internet Engineering Task Force RFC or a normalized specification through another standards body. Typical requirements include real-world usage in multiple places, complete documentation, no patent encumbrance, and at least two independent implementations.","title":"Standards Compatible"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#existing-robotics-recording-formats","text":"We can compare this set of requirements and desirable properties against existing file formats designed as robot recording containers.","title":"Existing Robotics Recording Formats"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#rosbag1","text":"The original ROS1 .bag file format meets several of our requirements. It is deterministic, self-describing, supports big and small data, somewhat write-optimized (although not append-only), does not require a priori channel knowledge, somewhat corruption resistant (no checksums, but data can be recovered at the chunk level and the index can be rebuilt), read-optimized via an index, and standards compatible. Its major shortcoming is that it is not serialization agnostic.","title":"rosbag1"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#rosbag2","text":"Whereas rosbag1 refers to both a file format definition and two reference implementations (C++ and Python), rosbag2 is a C API with a plugin interface for implementations, plus a default implementation using SQLite as the on-disk format. Since this document is focused on serialization formats, rosbag2 will be defined as the default SQLite implementation. This format is serialization agnostic, deterministic, but not (yet) self-describing, although there is ongoing work to address that. It does not require a priori channel knowledge, is corruption resistant , and is read-optimized except in the web use case. The significant shortcomings are around write-optimization (not append-only, the index is updated per row insert), lack of compression support at write time beyond the individual message level, and requiring the entire file to be read into memory in the web use case (via sql.js ). SQLite also poses challenges to standardization. The Internet Engineering Task Force, W3C, and other standards bodies require any specification to have two independent implementations to graduate to a standard. SQLite has only ever had a single implementation that is cross-compiled to different platforms, which caused the Web SQL Database proposal to reach an impasse .","title":"rosbag2"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#length-delimited-protobuf","text":"From interviewing various robotics companies, the most popular serialization format outside of ROS is to write protobuf messages to disk with a simple length delimiter or short message header. If timestamps are included in every message, this can be deterministic, but the initial .proto schemas must be bundled as well to meet the self-describing criteria. This approach does not require a priori channel knowledge and supports big and small data. It does not provide any explicit corruption resistance, although a truncated final message can sometimes be detected by a serialization failure. It is a write-optimized append-only approach, and compression can be supported by piping the write output through a compressor. Protobuf has been submitted as an Internet-Draft (I-D) standard. The significant shortcomings are that by nature of defining around a single serialization format, it is not serialization agnostic, and that without a well-defined standard, the files produced by different implementations will be incompatible. The lack of read-optimization excludes its use in playback or analysis tools that require random seeking.","title":"Length-delimited Protobuf"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#apollo-cyber-rt","text":"Cyber RT includes its own recording file format. However, the documentation of the format itself (opposed to the software APIs surrounding it) is scarce. It is likely similar to Length-delimited Protobuf for the purpose of this discussion.","title":"Apollo Cyber RT"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#existing-big-data-formats","text":"Outside of robotic recording file formats, we can evaluate common \u201cbig data\u201d containers for suitability to this problem.","title":"Existing Big Data Formats"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#avro-native-types","text":"Avro is a row-based storage format that requires each row to follow a single schema. This requires a union type of all possible message schemas that need to be rewritten as new channels are added into a container or each channel to be written to a separate file. Support for different serialization formats requires mapping each serialization format to Avro data types, which may be a lossy process.","title":"Avro - Native Types"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#avro-opaque-bytes","text":"Another way of using the Avro format is to define the schema as a single field containing an opaque array of bytes, which supports different serialization formats in a common container. This approach meets many of the desired requirements, but it defeats the most commonly desired benefit of using an existing container format: the ability to ingest data into a third-party system and have that system understand the data in a typed way. A recording format built on Avro would still require standardizing how and where to store content types and per-channel information such as schemas. Regardless, Avro containers storing opaque byte messages may be one possible solution.","title":"Avro - Opaque Bytes"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#hdf5","text":"HDF5 predates Avro and suffers from similar limitations while also missing support for deeply nested data via schemas. Using HDF5 as strictly a wrapper around opaque byte messages would require a more complex file format for no additional gain.","title":"HDF5"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#parquet","text":"Parquet is a columnar data store that is not suitable for random access to messages and is not optimized for replay. While Parquet can play a valuable role in robotics for analysis and long-term storage, its lack of suitability for real-time recording and message playback excludes it from our search for a recording file format.","title":"Parquet"},{"location":"motivation/evaluation-of-robotics-data-recording-file-formats.html#next-steps","text":"By comparing existing robotics recording formats with generic big data container formats, we can see there are practical advantages to designing a recording format specific to the robotics use case. However, none of the existing recording formats meet all the requirements and desirable traits laid out in this document. Many of the proposed approaches (rosbag2 SQLite, Avro with opaque buffer) follow an anti-pattern of using an existing schema-based format but only storing opaque binary blobs, negating the promise of interoperability from using an existing format. In light of this evaluation, I would like to propose a file format tailored to the requirements of robotics recording. I will introduce this file format in a follow-up proposal. The goal is to continue collecting requirements and desirable traits and collaboratively craft an industry standard backed by multiple implementations.","title":"Next Steps"},{"location":"specification/index.html","text":"MCAP File Format Specification Overview MCAP is a modular container file format for recording timestamped pub/sub messages with arbitrary serialization formats. MCAP files are designed to work well under various workloads, resource constraints, and durability requirements. A Kaitai Struct description for the MCAP format is provided at mcap.ksy . File Structure A valid MCAP file is structured as follows. The Summary and Summary Offset sections are optional. <Magic><Header><Data section>[<Summary section>][<Summary Offset section>]<Footer><Magic> The Data, Summary, and Summary Offset sections are structured as sequences of records : [<record type><record content length><record><record type><record content length><record>...] Files not conforming to this structure are considered malformed. Magic An MCAP file must begin and end with the following magic bytes : 0x89, M, C, A, P, 0x30, \\r, \\n The byte following \u201cMCAP\u201d is the major version byte. 0x30 is the ASCII character 0 . Any changes to this specification document (i.e. adding fields to records, introducing new records) will be binary backward-compatible within the major version. Header The first record after the leading magic bytes is the Header record. <0x01><record content length><record> Footer The last record before the trailing magic bytes is the Footer record. <0x02><record content length><record> Data Section The data section contains records with message data, attachments, and supporting records. The following records are allowed to appear in the data section: Schema Channel Message Attachment Chunk Message Index Metadata Data End The last record in the data section MUST be the Data End record. Summary Section The optional summary section contains records for fast lookup of file information or other data section records. The following records are allowed to appear in the summary section: Schema Channel Chunk Index Attachment Index Statistics Metadata Index All records in the summary section MUST be grouped by opcode. Why? Grouping Summary records by record opcode enables more efficient indexing of the summary in the Summary Offset section. Channel records in the summary are duplicates of Channel records throughout the Data section. Schema records in the summary are duplicates of Schema records throughout the Data section. Summary Offset Section The optional summary offset section contains Summary Offset records for fast lookup of summary section records. The summary offset section aids random access reading. Records MCAP files may contain a variety of records. Records are identified by a single-byte opcode . Record opcodes in the range 0x01-0x7F are reserved for future MCAP format usage. 0x80-0xFF are reserved for application extensions and user proposals. All MCAP records are serialized as follows: <record type><record content length><record content> Record type is a single byte opcode, and record content length is a uint64 value. Records may be extended by adding new fields at the end of existing fields. Readers should ignore any unknown fields. The Footer and Message records will not be extended, since their formats do not allow for backward-compatible size changes. Each record definition below contains a Type column. See the Serialization section on how to serialize each type. Header (op=0x01) Bytes Name Type Description 4 + N profile String The profile is used for indicating requirements for fields throughout the file (encoding, user_data, etc). If the value matches one of the well-known profiles , the file should conform to the profile. This field may also be supplied empty or containing a framework that is not one of those recognized. 4 + N library String Free-form string for writer to specify its name, version, or other information for use in debugging Footer (op=0x02) A Footer record contains end-of-file information. It must be the last record in the file. Readers using the index to read the file will begin with by reading the footer and trailing magic. Bytes Name Type Description 8 summary_start uint64 Byte offset of the start of file to the first record in the summary section. If there are no records in the summary section this should be 0. 8 summary_offset_start uint64 Byte offset from the start of the first record in the summary offset section. If there are no Summary Offset records this value should be 0. 4 summary_crc uint32 A CRC32 of all bytes from the start of the Summary section up through and including the end of the previous field (summary_offset_start) in the footer record. A value of 0 indicates the CRC32 is not available. Schema (op=0x03) A Schema record defines an individual schema. Schema records are uniquely identified within a file by their schema ID. A Schema record must occur at least once in the file prior to any Channel referring to its ID. Any two schema records sharing a common ID must be identical. Bytes Name Type Description 2 id uint16 A unique identifier for this schema within the file. Must not be zero 4 + N name String An identifier for the schema. 4 + N encoding String Format for the schema. The well-known schema encodings are preferred. An empty string indicates no schema is available. 4 + N data uint32 length-prefixed Bytes Must conform to the schema encoding. If encoding is an empty string, data should be 0 length. Schema records may be duplicated in the summary section. A Schema record with an id of zero is invalid and should be ignored by readers. Channel (op=0x04) A Channel record defines an encoded stream of messages on a topic. Channel records are uniquely identified within a file by their channel ID. A Channel record must occur at least once in the file prior to any message referring to its channel ID. Any two channel records sharing a common ID must be identical. Bytes Name Type Description 2 id uint16 A unique identifier for this channel within the file. 2 schema_id uint16 The schema for messages on this channel. A schema_id of 0 indicates there is no schema for this channel. 4 + N topic String The channel topic. 4 + N message_encoding String Encoding for messages on this channel. The well-known message encodings are preferred. 4 + N metadata Map<string, string> Metadata about this channel Channel records may be duplicated in the summary section. Message (op=0x05) A message record encodes a single timestamped message on a channel. The message encoding and schema must match that of the Channel record corresponding to the message\u2019s channel ID. Bytes Name Type Description 2 channel_id uint16 Channel ID 4 sequence uint32 Optional message counter assigned by publisher. If not assigned by publisher, must be recorded by the recorder. 8 log_time Timestamp Time at which the message was recorded. 8 publish_time Timestamp Time at which the message was published. If not available, must be set to the log time. N data Bytes Message data, to be decoded according to the schema of the channel. Chunk (op=0x06) A Chunk contains a batch of Schema, Channel, and Message records. The batch of records contained in a chunk may be compressed or uncompressed. All messages in the chunk must reference channels recorded earlier in the file (in a previous chunk or earlier in the current chunk). Bytes Name Type Description 8 message_start_time Timestamp Earliest message log_time in the chunk. Zero if the chunk has no messages. 8 message_end_time Timestamp Latest message log_time in the chunk. Zero if the chunk has no messages. 8 uncompressed_size uint64 Uncompressed size of the records field. 4 uncompressed_crc uint32 CRC32 checksum of uncompressed records field. A value of zero indicates that CRC validation should not be performed. 4 + N compression String compression algorithm. i.e. zstd , lz4 , \"\" . An empty string indicates no compression. Refer to well-known compression formats . 8 + N records uint64 length-prefixed Bytes Repeating sequences of <record type><record content length><record content> . Compressed with the algorithm in the compression field. Message Index (op=0x07) A Message Index record allows readers to locate individual message records within a chunk by their timestamp. A sequence of Message Index records occurs immediately after each chunk. Exactly one Message Index record must exist in the sequence for every channel on which a message occurs inside the chunk. Bytes Name Type Description 2 channel_id uint16 Channel ID. 4 + N records Array<Tuple<Timestamp, uint64>> Array of log_time and offset for each record. Offset is relative to the start of the uncompressed chunk data. Messages outside of chunks cannot be indexed. Chunk Index (op=0x08) A Chunk Index record contains the location of a Chunk record and its associated Message Index records. A Chunk Index record exists for every Chunk in the file. Bytes Name Type Description 8 message_start_time Timestamp Earliest message log_time in the chunk. Zero if the chunk has no messages. 8 message_end_time Timestamp Latest message log_time in the chunk. Zero if the chunk has no messages. 8 chunk_start_offset uint64 Offset to the chunk record from the start of the file. 8 chunk_length uint64 Byte length of the chunk record, including opcode and length prefix. 4 + N message_index_offsets Map<uint16, uint64> Mapping from channel ID to the offset of the message index record for that channel after the chunk, from the start of the file. An empty map indicates no message indexing is available. 8 message_index_length uint64 Total length in bytes of the message index records after the chunk. 4 + N compression String The compression used within the chunk. Refer to well-known compression formats . This field should match the the value in the corresponding Chunk record. 8 compressed_size uint64 The size of the chunk records field. 8 uncompressed_size uint64 The uncompressed size of the chunk records field. This field should match the value in the corresponding Chunk record. A Schema and Channel record MUST exist in the summary section for all channels referenced by chunk index records. Why? The typical use case for file readers using an index is fast random access to a specific message timestamp. Channel is a prerequisite for decoding Message record data. Without an easy-to-access copy of the Channel records, readers would need to search for Channel records from the start of the file, degrading random access read performance. Attachment (op=0x09) Attachment records contain auxiliary artifacts such as text, core dumps, calibration data, or other arbitrary data. Attachment records must not appear within a chunk. Bytes Name Type Description 8 log_time Timestamp Time at which the attachment was recorded. 8 create_time Timestamp Time at which the attachment was created. If not available, must be set to zero. 4 + N name String Name of the attachment, e.g \u201cscene1.jpg\u201d. 4 + N media_type String Media type (e.g \u201ctext/plain\u201d). 8 + N data uint64 length-prefixed Bytes Attachment data. 4 crc uint32 CRC32 checksum of preceding fields in the record. A value of zero indicates that CRC validation should not be performed. Attachment Index (op=0x0A) An Attachment Index record contains the location of an attachment in the file. An Attachment Index record exists for every Attachment record in the file. Bytes Name Type Description 8 offset uint64 Byte offset from the start of the file to the attachment record. 8 length uint64 Byte length of the attachment record, including opcode and length prefix. 8 log_time Timestamp Time at which the attachment was recorded. 8 create_time Timestamp Time at which the attachment was created. If not available, must be set to zero. 8 data_size uint64 Size of the attachment data. 4 + N name String Name of the attachment. 4 + N media_type String Media type of the attachment (e.g \u201ctext/plain\u201d). Statistics (op=0x0B) A Statistics record contains summary information about the recorded data. The statistics record is optional, but the file should contain at most one. Bytes Name Type Description 8 message_count uint64 Number of Message records in the file. 2 schema_count uint16 Number of unique schema IDs in the file, not including zero. 4 channel_count uint32 Number of unique channel IDs in the file. 4 attachment_count uint32 Number of Attachment records in the file. 4 metadata_count uint32 Number of Metadata records in the file. 4 chunk_count uint32 Number of Chunk records in the file. 8 message_start_time Timestamp Earliest message log_time in the file. Zero if the file has no messages. 8 message_end_time Timestamp Latest message log_time in the file. Zero if the file has no messages. 4 + N channel_message_counts Map<uint16, uint64> Mapping from channel ID to total message count for the channel. An empty map indicates this statistic is not available. When using a Statistics record with a non-empty channel_message_counts, the Summary Data section MUST contain a copy of all Channel records. The Channel records MUST occur prior to the statistics record. Why? The typical use case for tools is to provide a listing of the types and quantities of messages stored in the file. Without an easy to access copy of the Channel records, tools would need to linearly scan the file for Channel records to display what types of messages exist in the file. Metadata (op=0x0C) A metadata record contains arbitrary user data in key-value pairs. Bytes Name Type Description 4 + N name String Example: map_metadata . 4 + N metadata Map<string, string> Example keys: robot_id , git_sha , timezone , run_id . Metadata Index (op=0x0D) A metadata index record contains the location of a metadata record within the file. Bytes Name Type Description 8 offset uint64 Byte offset from the start of the file to the metadata record. 8 length uint64 Total byte length of the record, including opcode and length prefix. 4 + N name String Name of the metadata record. Summary Offset (op=0x0E) A Summary Offset record contains the location of records within the summary section. Each Summary Offset record corresponds to a group of summary records with the same opcode. Bytes Name Type Description 1 group_opcode uint8 The opcode of all records in the group. 8 group_start uint64 Byte offset from the start of the file of the first record in the group. 8 group_length uint64 Total byte length of all records in the group. Data End (op=0x0F) A Data End record indicates the end of the data section. Why? When reading a file from start to end, there is ambiguity when the data section ends and the summary section starts because some records (i.e. Channel) can repeat for summary data. The Data End record provides a clear delineation the data section has ended. Bytes Name Type Description 4 data_section_crc uint32 CRC32 of all bytes from the beginning of the file up to the DataEnd record. A value of 0 indicates the CRC32 is not available. Serialization Fixed-width types Multi-byte integers ( uint16 , uint32 , uint64 ) are serialized using little-endian byte order . String Strings are serialized using a uint32 byte length followed by the string data, which should be valid UTF-8 . <byte length><utf-8 bytes> Bytes Bytes is sequence of bytes with no additional requirements. <bytes> Tuple<first_type, second_type> Tuple represents a pair of values. The first value has type first_type and the second has type second_type. Tuple is serialized by serializing the first value and then the second value: <first value><second value> A Tuple<uint8, uint32> : <uint8><uint32> A Tuple<uint16, string> : <uint16><string> <uint16><uint32><utf-8 bytes> Array<array_type> Arrays are serialized using a uint32 byte length followed by the serialized array elements. <byte length><serialized element><serialized element>... An array of uint64 is specified as Array<uint64> and serialized as: <byte length><uint64><uint64><uint64>... Since arrays use a uint32 byte length prefix, the maximum size of the serialized array elements cannot exceed 4,294,967,295 bytes. Timestamp uint64 nanoseconds since a user-understood epoch (i.e unix epoch, robot boot time, etc.) Map<key_type, value_type> A Map is an association of unique keys to values. Maps are serialized using a uint32 byte length followed by the serialized map key/value entries. The key and value entries are serialized according to their key_type and value_type . <byte length><key><value><key><value>... A Map<string, string> would be serialized as: <byte length><uint32 key length><utf-8 key bytes><uint32 value length><utf-8 value bytes>... A serialization which has duplicate keys may cause indeterminate decoding. Diagrams The following diagrams demonstrate various valid MCAP files. Empty file The smallest valid MCAP file, containing no data. [Header] [Footer] Single Message An MCAP file containing 1 message. [Header] [Schema A] [Channel 1 (A)] [Message on Channel 1] [Footer] Single Attachment An MCAP file containing 1 attachment [Header] [Attachment] [Footer] Multiple Messages [Header] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Schema B] [Channel 3 (B)] [Attachment] [Message on 3] [Message on 1] [Footer] Messages in Chunks A writer may choose to put messages in Chunks to compress record data. This MCAP file does not use any index records. [Header] [Chunk] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Attachment] [Chunk] [Schema B] [Channel 3 (B)] [Message on 3] [Message on 1] [Footer] Multiple Messages with Summary Data [Header] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Schema B] [Channel 3 (B)] [Attachment] [Message on 3] [Message on 1] [Data End] [Statistics] [Schema A] [Schema B] [Channel 1] [Channel 2] [Channel 3] [Summary Offset 0x01] [Footer] Multiple Messages with Chunk Indices [Header] [Chunk A] [Schema A] [Channel 1 (A)] [Channel 2 (B)] [Message on 1] [Message on 1] [Message on 2] [Message Index 1] [Message Index 2] [Attachment 1] [Chunk B] [Schema B] [Channel 3 (B)] [Message on 3] [Message on 1] [Message Index 3] [Message Index 1] [Data End] [Schema A] [Schema B] [Channel 1] [Channel 2] [Channel 3] [Chunk Index A] [Chunk Index B] [Attachment Index 1] [Statistics] [Summary Offset 0x01] [Summary Offset 0x05] [Summary Offset 0x07] [Summary Offset 0x08] [Footer] Further Reading Feature explanations : includes usage details that may be useful to implementers of readers or writers.","title":"MCAP File Format Specification"},{"location":"specification/index.html#mcap-file-format-specification","text":"","title":"MCAP File Format Specification"},{"location":"specification/index.html#overview","text":"MCAP is a modular container file format for recording timestamped pub/sub messages with arbitrary serialization formats. MCAP files are designed to work well under various workloads, resource constraints, and durability requirements. A Kaitai Struct description for the MCAP format is provided at mcap.ksy .","title":"Overview"},{"location":"specification/index.html#file-structure","text":"A valid MCAP file is structured as follows. The Summary and Summary Offset sections are optional. <Magic><Header><Data section>[<Summary section>][<Summary Offset section>]<Footer><Magic> The Data, Summary, and Summary Offset sections are structured as sequences of records : [<record type><record content length><record><record type><record content length><record>...] Files not conforming to this structure are considered malformed.","title":"File Structure"},{"location":"specification/index.html#magic","text":"An MCAP file must begin and end with the following magic bytes : 0x89, M, C, A, P, 0x30, \\r, \\n The byte following \u201cMCAP\u201d is the major version byte. 0x30 is the ASCII character 0 . Any changes to this specification document (i.e. adding fields to records, introducing new records) will be binary backward-compatible within the major version.","title":"Magic"},{"location":"specification/index.html#header","text":"The first record after the leading magic bytes is the Header record. <0x01><record content length><record>","title":"Header"},{"location":"specification/index.html#footer","text":"The last record before the trailing magic bytes is the Footer record. <0x02><record content length><record>","title":"Footer"},{"location":"specification/index.html#data-section","text":"The data section contains records with message data, attachments, and supporting records. The following records are allowed to appear in the data section: Schema Channel Message Attachment Chunk Message Index Metadata Data End The last record in the data section MUST be the Data End record.","title":"Data Section"},{"location":"specification/index.html#summary-section","text":"The optional summary section contains records for fast lookup of file information or other data section records. The following records are allowed to appear in the summary section: Schema Channel Chunk Index Attachment Index Statistics Metadata Index All records in the summary section MUST be grouped by opcode. Why? Grouping Summary records by record opcode enables more efficient indexing of the summary in the Summary Offset section. Channel records in the summary are duplicates of Channel records throughout the Data section. Schema records in the summary are duplicates of Schema records throughout the Data section.","title":"Summary Section"},{"location":"specification/index.html#summary-offset-section","text":"The optional summary offset section contains Summary Offset records for fast lookup of summary section records. The summary offset section aids random access reading.","title":"Summary Offset Section"},{"location":"specification/index.html#records","text":"MCAP files may contain a variety of records. Records are identified by a single-byte opcode . Record opcodes in the range 0x01-0x7F are reserved for future MCAP format usage. 0x80-0xFF are reserved for application extensions and user proposals. All MCAP records are serialized as follows: <record type><record content length><record content> Record type is a single byte opcode, and record content length is a uint64 value. Records may be extended by adding new fields at the end of existing fields. Readers should ignore any unknown fields. The Footer and Message records will not be extended, since their formats do not allow for backward-compatible size changes. Each record definition below contains a Type column. See the Serialization section on how to serialize each type.","title":"Records"},{"location":"specification/index.html#header-op0x01","text":"Bytes Name Type Description 4 + N profile String The profile is used for indicating requirements for fields throughout the file (encoding, user_data, etc). If the value matches one of the well-known profiles , the file should conform to the profile. This field may also be supplied empty or containing a framework that is not one of those recognized. 4 + N library String Free-form string for writer to specify its name, version, or other information for use in debugging","title":"Header (op=0x01)"},{"location":"specification/index.html#footer-op0x02","text":"A Footer record contains end-of-file information. It must be the last record in the file. Readers using the index to read the file will begin with by reading the footer and trailing magic. Bytes Name Type Description 8 summary_start uint64 Byte offset of the start of file to the first record in the summary section. If there are no records in the summary section this should be 0. 8 summary_offset_start uint64 Byte offset from the start of the first record in the summary offset section. If there are no Summary Offset records this value should be 0. 4 summary_crc uint32 A CRC32 of all bytes from the start of the Summary section up through and including the end of the previous field (summary_offset_start) in the footer record. A value of 0 indicates the CRC32 is not available.","title":"Footer (op=0x02)"},{"location":"specification/index.html#schema-op0x03","text":"A Schema record defines an individual schema. Schema records are uniquely identified within a file by their schema ID. A Schema record must occur at least once in the file prior to any Channel referring to its ID. Any two schema records sharing a common ID must be identical. Bytes Name Type Description 2 id uint16 A unique identifier for this schema within the file. Must not be zero 4 + N name String An identifier for the schema. 4 + N encoding String Format for the schema. The well-known schema encodings are preferred. An empty string indicates no schema is available. 4 + N data uint32 length-prefixed Bytes Must conform to the schema encoding. If encoding is an empty string, data should be 0 length. Schema records may be duplicated in the summary section. A Schema record with an id of zero is invalid and should be ignored by readers.","title":"Schema (op=0x03)"},{"location":"specification/index.html#channel-op0x04","text":"A Channel record defines an encoded stream of messages on a topic. Channel records are uniquely identified within a file by their channel ID. A Channel record must occur at least once in the file prior to any message referring to its channel ID. Any two channel records sharing a common ID must be identical. Bytes Name Type Description 2 id uint16 A unique identifier for this channel within the file. 2 schema_id uint16 The schema for messages on this channel. A schema_id of 0 indicates there is no schema for this channel. 4 + N topic String The channel topic. 4 + N message_encoding String Encoding for messages on this channel. The well-known message encodings are preferred. 4 + N metadata Map<string, string> Metadata about this channel Channel records may be duplicated in the summary section.","title":"Channel (op=0x04)"},{"location":"specification/index.html#message-op0x05","text":"A message record encodes a single timestamped message on a channel. The message encoding and schema must match that of the Channel record corresponding to the message\u2019s channel ID. Bytes Name Type Description 2 channel_id uint16 Channel ID 4 sequence uint32 Optional message counter assigned by publisher. If not assigned by publisher, must be recorded by the recorder. 8 log_time Timestamp Time at which the message was recorded. 8 publish_time Timestamp Time at which the message was published. If not available, must be set to the log time. N data Bytes Message data, to be decoded according to the schema of the channel.","title":"Message (op=0x05)"},{"location":"specification/index.html#chunk-op0x06","text":"A Chunk contains a batch of Schema, Channel, and Message records. The batch of records contained in a chunk may be compressed or uncompressed. All messages in the chunk must reference channels recorded earlier in the file (in a previous chunk or earlier in the current chunk). Bytes Name Type Description 8 message_start_time Timestamp Earliest message log_time in the chunk. Zero if the chunk has no messages. 8 message_end_time Timestamp Latest message log_time in the chunk. Zero if the chunk has no messages. 8 uncompressed_size uint64 Uncompressed size of the records field. 4 uncompressed_crc uint32 CRC32 checksum of uncompressed records field. A value of zero indicates that CRC validation should not be performed. 4 + N compression String compression algorithm. i.e. zstd , lz4 , \"\" . An empty string indicates no compression. Refer to well-known compression formats . 8 + N records uint64 length-prefixed Bytes Repeating sequences of <record type><record content length><record content> . Compressed with the algorithm in the compression field.","title":"Chunk (op=0x06)"},{"location":"specification/index.html#message-index-op0x07","text":"A Message Index record allows readers to locate individual message records within a chunk by their timestamp. A sequence of Message Index records occurs immediately after each chunk. Exactly one Message Index record must exist in the sequence for every channel on which a message occurs inside the chunk. Bytes Name Type Description 2 channel_id uint16 Channel ID. 4 + N records Array<Tuple<Timestamp, uint64>> Array of log_time and offset for each record. Offset is relative to the start of the uncompressed chunk data. Messages outside of chunks cannot be indexed.","title":"Message Index (op=0x07)"},{"location":"specification/index.html#chunk-index-op0x08","text":"A Chunk Index record contains the location of a Chunk record and its associated Message Index records. A Chunk Index record exists for every Chunk in the file. Bytes Name Type Description 8 message_start_time Timestamp Earliest message log_time in the chunk. Zero if the chunk has no messages. 8 message_end_time Timestamp Latest message log_time in the chunk. Zero if the chunk has no messages. 8 chunk_start_offset uint64 Offset to the chunk record from the start of the file. 8 chunk_length uint64 Byte length of the chunk record, including opcode and length prefix. 4 + N message_index_offsets Map<uint16, uint64> Mapping from channel ID to the offset of the message index record for that channel after the chunk, from the start of the file. An empty map indicates no message indexing is available. 8 message_index_length uint64 Total length in bytes of the message index records after the chunk. 4 + N compression String The compression used within the chunk. Refer to well-known compression formats . This field should match the the value in the corresponding Chunk record. 8 compressed_size uint64 The size of the chunk records field. 8 uncompressed_size uint64 The uncompressed size of the chunk records field. This field should match the value in the corresponding Chunk record. A Schema and Channel record MUST exist in the summary section for all channels referenced by chunk index records. Why? The typical use case for file readers using an index is fast random access to a specific message timestamp. Channel is a prerequisite for decoding Message record data. Without an easy-to-access copy of the Channel records, readers would need to search for Channel records from the start of the file, degrading random access read performance.","title":"Chunk Index (op=0x08)"},{"location":"specification/index.html#attachment-op0x09","text":"Attachment records contain auxiliary artifacts such as text, core dumps, calibration data, or other arbitrary data. Attachment records must not appear within a chunk. Bytes Name Type Description 8 log_time Timestamp Time at which the attachment was recorded. 8 create_time Timestamp Time at which the attachment was created. If not available, must be set to zero. 4 + N name String Name of the attachment, e.g \u201cscene1.jpg\u201d. 4 + N media_type String Media type (e.g \u201ctext/plain\u201d). 8 + N data uint64 length-prefixed Bytes Attachment data. 4 crc uint32 CRC32 checksum of preceding fields in the record. A value of zero indicates that CRC validation should not be performed.","title":"Attachment (op=0x09)"},{"location":"specification/index.html#attachment-index-op0x0a","text":"An Attachment Index record contains the location of an attachment in the file. An Attachment Index record exists for every Attachment record in the file. Bytes Name Type Description 8 offset uint64 Byte offset from the start of the file to the attachment record. 8 length uint64 Byte length of the attachment record, including opcode and length prefix. 8 log_time Timestamp Time at which the attachment was recorded. 8 create_time Timestamp Time at which the attachment was created. If not available, must be set to zero. 8 data_size uint64 Size of the attachment data. 4 + N name String Name of the attachment. 4 + N media_type String Media type of the attachment (e.g \u201ctext/plain\u201d).","title":"Attachment Index (op=0x0A)"},{"location":"specification/index.html#statistics-op0x0b","text":"A Statistics record contains summary information about the recorded data. The statistics record is optional, but the file should contain at most one. Bytes Name Type Description 8 message_count uint64 Number of Message records in the file. 2 schema_count uint16 Number of unique schema IDs in the file, not including zero. 4 channel_count uint32 Number of unique channel IDs in the file. 4 attachment_count uint32 Number of Attachment records in the file. 4 metadata_count uint32 Number of Metadata records in the file. 4 chunk_count uint32 Number of Chunk records in the file. 8 message_start_time Timestamp Earliest message log_time in the file. Zero if the file has no messages. 8 message_end_time Timestamp Latest message log_time in the file. Zero if the file has no messages. 4 + N channel_message_counts Map<uint16, uint64> Mapping from channel ID to total message count for the channel. An empty map indicates this statistic is not available. When using a Statistics record with a non-empty channel_message_counts, the Summary Data section MUST contain a copy of all Channel records. The Channel records MUST occur prior to the statistics record. Why? The typical use case for tools is to provide a listing of the types and quantities of messages stored in the file. Without an easy to access copy of the Channel records, tools would need to linearly scan the file for Channel records to display what types of messages exist in the file.","title":"Statistics (op=0x0B)"},{"location":"specification/index.html#metadata-op0x0c","text":"A metadata record contains arbitrary user data in key-value pairs. Bytes Name Type Description 4 + N name String Example: map_metadata . 4 + N metadata Map<string, string> Example keys: robot_id , git_sha , timezone , run_id .","title":"Metadata (op=0x0C)"},{"location":"specification/index.html#metadata-index-op0x0d","text":"A metadata index record contains the location of a metadata record within the file. Bytes Name Type Description 8 offset uint64 Byte offset from the start of the file to the metadata record. 8 length uint64 Total byte length of the record, including opcode and length prefix. 4 + N name String Name of the metadata record.","title":"Metadata Index (op=0x0D)"},{"location":"specification/index.html#summary-offset-op0x0e","text":"A Summary Offset record contains the location of records within the summary section. Each Summary Offset record corresponds to a group of summary records with the same opcode. Bytes Name Type Description 1 group_opcode uint8 The opcode of all records in the group. 8 group_start uint64 Byte offset from the start of the file of the first record in the group. 8 group_length uint64 Total byte length of all records in the group.","title":"Summary Offset (op=0x0E)"},{"location":"specification/index.html#data-end-op0x0f","text":"A Data End record indicates the end of the data section. Why? When reading a file from start to end, there is ambiguity when the data section ends and the summary section starts because some records (i.e. Channel) can repeat for summary data. The Data End record provides a clear delineation the data section has ended. Bytes Name Type Description 4 data_section_crc uint32 CRC32 of all bytes from the beginning of the file up to the DataEnd record. A value of 0 indicates the CRC32 is not available.","title":"Data End (op=0x0F)"},{"location":"specification/index.html#serialization","text":"","title":"Serialization"},{"location":"specification/index.html#fixed-width-types","text":"Multi-byte integers ( uint16 , uint32 , uint64 ) are serialized using little-endian byte order .","title":"Fixed-width types"},{"location":"specification/index.html#string","text":"Strings are serialized using a uint32 byte length followed by the string data, which should be valid UTF-8 . <byte length><utf-8 bytes>","title":"String"},{"location":"specification/index.html#bytes","text":"Bytes is sequence of bytes with no additional requirements. <bytes>","title":"Bytes"},{"location":"specification/index.html#tuplefirst_type-second_type","text":"Tuple represents a pair of values. The first value has type first_type and the second has type second_type. Tuple is serialized by serializing the first value and then the second value: <first value><second value> A Tuple<uint8, uint32> : <uint8><uint32> A Tuple<uint16, string> : <uint16><string> <uint16><uint32><utf-8 bytes>","title":"Tuple&lt;first_type, second_type&gt;"},{"location":"specification/index.html#arrayarray_type","text":"Arrays are serialized using a uint32 byte length followed by the serialized array elements. <byte length><serialized element><serialized element>... An array of uint64 is specified as Array<uint64> and serialized as: <byte length><uint64><uint64><uint64>... Since arrays use a uint32 byte length prefix, the maximum size of the serialized array elements cannot exceed 4,294,967,295 bytes.","title":"Array&lt;array_type&gt;"},{"location":"specification/index.html#timestamp","text":"uint64 nanoseconds since a user-understood epoch (i.e unix epoch, robot boot time, etc.)","title":"Timestamp"},{"location":"specification/index.html#mapkey_type-value_type","text":"A Map is an association of unique keys to values. Maps are serialized using a uint32 byte length followed by the serialized map key/value entries. The key and value entries are serialized according to their key_type and value_type . <byte length><key><value><key><value>... A Map<string, string> would be serialized as: <byte length><uint32 key length><utf-8 key bytes><uint32 value length><utf-8 value bytes>... A serialization which has duplicate keys may cause indeterminate decoding.","title":"Map&lt;key_type, value_type&gt;"},{"location":"specification/index.html#diagrams","text":"The following diagrams demonstrate various valid MCAP files.","title":"Diagrams"},{"location":"specification/index.html#empty-file","text":"The smallest valid MCAP file, containing no data. [Header] [Footer]","title":"Empty file"},{"location":"specification/index.html#single-message","text":"An MCAP file containing 1 message. [Header] [Schema A] [Channel 1 (A)] [Message on Channel 1] [Footer]","title":"Single Message"},{"location":"specification/index.html#single-attachment","text":"An MCAP file containing 1 attachment [Header] [Attachment] [Footer]","title":"Single Attachment"},{"location":"specification/index.html#multiple-messages","text":"[Header] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Schema B] [Channel 3 (B)] [Attachment] [Message on 3] [Message on 1] [Footer]","title":"Multiple Messages"},{"location":"specification/index.html#messages-in-chunks","text":"A writer may choose to put messages in Chunks to compress record data. This MCAP file does not use any index records. [Header] [Chunk] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Attachment] [Chunk] [Schema B] [Channel 3 (B)] [Message on 3] [Message on 1] [Footer]","title":"Messages in Chunks"},{"location":"specification/index.html#multiple-messages-with-summary-data","text":"[Header] [Schema A] [Channel 1 (A)] [Channel 2 (A)] [Message on 1] [Message on 1] [Message on 2] [Schema B] [Channel 3 (B)] [Attachment] [Message on 3] [Message on 1] [Data End] [Statistics] [Schema A] [Schema B] [Channel 1] [Channel 2] [Channel 3] [Summary Offset 0x01] [Footer]","title":"Multiple Messages with Summary Data"},{"location":"specification/index.html#multiple-messages-with-chunk-indices","text":"[Header] [Chunk A] [Schema A] [Channel 1 (A)] [Channel 2 (B)] [Message on 1] [Message on 1] [Message on 2] [Message Index 1] [Message Index 2] [Attachment 1] [Chunk B] [Schema B] [Channel 3 (B)] [Message on 3] [Message on 1] [Message Index 3] [Message Index 1] [Data End] [Schema A] [Schema B] [Channel 1] [Channel 2] [Channel 3] [Chunk Index A] [Chunk Index B] [Attachment Index 1] [Statistics] [Summary Offset 0x01] [Summary Offset 0x05] [Summary Offset 0x07] [Summary Offset 0x08] [Footer]","title":"Multiple Messages with Chunk Indices"},{"location":"specification/index.html#further-reading","text":"Feature explanations : includes usage details that may be useful to implementers of readers or writers.","title":"Further Reading"},{"location":"specification/appendix.html","text":"MCAP File Format Specification Appendix This document describes well known values for MCAP files. Well-known compression formats The Chunk compression field may contain the following options: lz4 : an algorithm that prioritizes compression/decompression speed over compression ratio. zstd : an algorithm with a tunable compression ratio/speed tradeoff. Well-known message encodings The Channel message_encoding field describes the encoding for all messages within a channel. This field is mandatory. ros1 message_encoding : ros1 cdr message_encoding : cdr (used in ROS 2) protobuf message_encoding : protobuf flatbuffer message_encoding : flatbuffer cbor message_encoding : cbor json message_encoding : json Well-known schema encodings The Schema encoding field describes the encoding of a Channel\u2019s schema. Typically, this is related to the Channel\u2019s message_encoding , but they are separate concepts (e.g. there are multiple schema languages for json ). This field is required for some message encodings (e.g. protobuf ) and optional for others (e.g. json ). (empty string) Schema encoding may only be omitted for self-describing message encodings such as json . name : May contain any value encoding : (empty string) data : Must be empty (0 bytes) protobuf name : Fully qualified name to the message within the descriptor set. For example, in a proto file containing package foo.bar; message Baz {} the fully qualified message name is foo.bar.Baz . encoding : protobuf data : A binary FileDescriptorSet as produced by protoc --descriptor_set_out . flatbuffer name : Fully qualified name corresponding to a name in the reflection Object table. For example in a schema containing namespace foo; table Bar {} the fully qualified name is foo.Bar . encoding : flatbuffer data : A reflection.Schema flatbuffer describing the parsed schema encoded as a binary flatbuffer. This can be generated by running flatc -b --schema on the fbs file. ros1msg name : A valid package resource name , e.g. sensor_msgs/PointCloud2 . encoding : ros1msg data : Delimited concatenated ROS1 .msg files. ros1msg Data Format the data field contains the concatenated .msg file content that is sent in the ROS subscription connection header for this message type. The top-level message definition is present first, with no delimiter. All dependent .msg definitions are preceded by a two-line delimiter: One line containing exactly 80 = characters One line containing MSG: <package resource name> for that type. The space between MSG: and the package resource name is mandatory. The package resource name does not include a file extension. This format can be reproduced using gendeps \u2013cat . ros2msg name : A valid package resource name , e.g. sensor_msgs/msg/PointCloud2 . encoding : ros2msg data : Delimited concatenated ROS2 .msg files ros2msg Data Format The .msg definition is stored alongside its dependencies in the same format as ros1msg . ros2idl name : A valid package resource name , e.g. sensor_msgs/msg/PointCloud2 encoding : ros2idl data : Delimited concatenated ROS2 .idl files ros2idl Data Format The IDL definition of the type specified by name along with all dependent types are stored together. The IDL definitions can be stored in any order. Every definition is preceded by a two-line delimiter: a line containing exactly 80 = characters, then A line containing only IDL: <package resource name> for that definition. The space between IDL: and the package resource name is mandatory. The package resource name does not include a file extension. jsonschema name : May contain any value encoding : jsonschema data : JSON Schema Well-known profiles ROS1 The ros1 profile describes how to create MCAP files for ROS 1 . Header profile : MUST contain ros1 Channel message_encoding : MUST contain ros metadata keys: callerid (optional, string) latching (optional, bool stringified as \u201ctrue\u201d or \u201cfalse\u201d) Schema encoding : MUST contain ros1msg ROS2 The ros2 profile describes how to create MCAP files for ROS 2 . Header profile : MUST contain ros2 Channel message_encoding : MUST contain cdr metadata : offered_qos_profiles (required, string) Schema encoding : MUST contain either ros2msg or ros2idl","title":"MCAP File Format Specification Appendix"},{"location":"specification/appendix.html#mcap-file-format-specification-appendix","text":"This document describes well known values for MCAP files.","title":"MCAP File Format Specification Appendix"},{"location":"specification/appendix.html#well-known-compression-formats","text":"The Chunk compression field may contain the following options: lz4 : an algorithm that prioritizes compression/decompression speed over compression ratio. zstd : an algorithm with a tunable compression ratio/speed tradeoff.","title":"Well-known compression formats"},{"location":"specification/appendix.html#well-known-message-encodings","text":"The Channel message_encoding field describes the encoding for all messages within a channel. This field is mandatory.","title":"Well-known message encodings"},{"location":"specification/appendix.html#ros1","text":"message_encoding : ros1","title":"ros1"},{"location":"specification/appendix.html#cdr","text":"message_encoding : cdr (used in ROS 2)","title":"cdr"},{"location":"specification/appendix.html#protobuf","text":"message_encoding : protobuf","title":"protobuf"},{"location":"specification/appendix.html#flatbuffer","text":"message_encoding : flatbuffer","title":"flatbuffer"},{"location":"specification/appendix.html#cbor","text":"message_encoding : cbor","title":"cbor"},{"location":"specification/appendix.html#json","text":"message_encoding : json","title":"json"},{"location":"specification/appendix.html#well-known-schema-encodings","text":"The Schema encoding field describes the encoding of a Channel\u2019s schema. Typically, this is related to the Channel\u2019s message_encoding , but they are separate concepts (e.g. there are multiple schema languages for json ). This field is required for some message encodings (e.g. protobuf ) and optional for others (e.g. json ).","title":"Well-known schema encodings"},{"location":"specification/appendix.html#empty-string","text":"Schema encoding may only be omitted for self-describing message encodings such as json . name : May contain any value encoding : (empty string) data : Must be empty (0 bytes)","title":"(empty string)"},{"location":"specification/appendix.html#protobuf_1","text":"name : Fully qualified name to the message within the descriptor set. For example, in a proto file containing package foo.bar; message Baz {} the fully qualified message name is foo.bar.Baz . encoding : protobuf data : A binary FileDescriptorSet as produced by protoc --descriptor_set_out .","title":"protobuf"},{"location":"specification/appendix.html#flatbuffer_1","text":"name : Fully qualified name corresponding to a name in the reflection Object table. For example in a schema containing namespace foo; table Bar {} the fully qualified name is foo.Bar . encoding : flatbuffer data : A reflection.Schema flatbuffer describing the parsed schema encoded as a binary flatbuffer. This can be generated by running flatc -b --schema on the fbs file.","title":"flatbuffer"},{"location":"specification/appendix.html#ros1msg","text":"name : A valid package resource name , e.g. sensor_msgs/PointCloud2 . encoding : ros1msg data : Delimited concatenated ROS1 .msg files.","title":"ros1msg"},{"location":"specification/appendix.html#ros1msg-data-format","text":"the data field contains the concatenated .msg file content that is sent in the ROS subscription connection header for this message type. The top-level message definition is present first, with no delimiter. All dependent .msg definitions are preceded by a two-line delimiter: One line containing exactly 80 = characters One line containing MSG: <package resource name> for that type. The space between MSG: and the package resource name is mandatory. The package resource name does not include a file extension. This format can be reproduced using gendeps \u2013cat .","title":"ros1msg Data Format"},{"location":"specification/appendix.html#ros2msg","text":"name : A valid package resource name , e.g. sensor_msgs/msg/PointCloud2 . encoding : ros2msg data : Delimited concatenated ROS2 .msg files","title":"ros2msg"},{"location":"specification/appendix.html#ros2msg-data-format","text":"The .msg definition is stored alongside its dependencies in the same format as ros1msg .","title":"ros2msg Data Format"},{"location":"specification/appendix.html#ros2idl","text":"name : A valid package resource name , e.g. sensor_msgs/msg/PointCloud2 encoding : ros2idl data : Delimited concatenated ROS2 .idl files","title":"ros2idl"},{"location":"specification/appendix.html#ros2idl-data-format","text":"The IDL definition of the type specified by name along with all dependent types are stored together. The IDL definitions can be stored in any order. Every definition is preceded by a two-line delimiter: a line containing exactly 80 = characters, then A line containing only IDL: <package resource name> for that definition. The space between IDL: and the package resource name is mandatory. The package resource name does not include a file extension.","title":"ros2idl Data Format"},{"location":"specification/appendix.html#jsonschema","text":"name : May contain any value encoding : jsonschema data : JSON Schema","title":"jsonschema"},{"location":"specification/appendix.html#well-known-profiles","text":"","title":"Well-known profiles"},{"location":"specification/appendix.html#ros1_1","text":"The ros1 profile describes how to create MCAP files for ROS 1 .","title":"ROS1"},{"location":"specification/appendix.html#header","text":"profile : MUST contain ros1","title":"Header"},{"location":"specification/appendix.html#channel","text":"message_encoding : MUST contain ros metadata keys: callerid (optional, string) latching (optional, bool stringified as \u201ctrue\u201d or \u201cfalse\u201d)","title":"Channel"},{"location":"specification/appendix.html#schema","text":"encoding : MUST contain ros1msg","title":"Schema"},{"location":"specification/appendix.html#ros2","text":"The ros2 profile describes how to create MCAP files for ROS 2 .","title":"ROS2"},{"location":"specification/appendix.html#header_1","text":"profile : MUST contain ros2","title":"Header"},{"location":"specification/appendix.html#channel_1","text":"message_encoding : MUST contain cdr metadata : offered_qos_profiles (required, string)","title":"Channel"},{"location":"specification/appendix.html#schema_1","text":"encoding : MUST contain either ros2msg or ros2idl","title":"Schema"},{"location":"specification/explanatory-notes.html","text":"Explanatory Notes The following notes may be useful for users of the MCAP format, including implementers of readers and writers. Feature Explanations The format is intended to support efficient, indexed reading of messages and generation of summary data in both local and remote contexts. \u201cSeeking\u201d should be imagined to incur either a disk seek or an HTTP range request to an object store \u2013 the latter being significantly more costly. In both random access and summarization, features may be unsupported due to choices taken by the writer of the file. For instance, statistics may not include channel message counts, or there may be no message index present. If the index data section is empty, the index_offset in the file footer will be set to zero. Scanning for records on specific topics within an interval The index is designed to support fast local and remote seek/filter operations with minimal seeking or range request overhead. The operation of the index for message reading is as follows: Client queries for all messages on topics /a, /b, /c between t0 and t1 Reader reads the fixed-length footer off the end of the file Reader parses the index_offset from the footer, and starts reading from that offset to the end of the file. During this read it will encounter the following in order: A run of Channel records, one per channel in the file A run of Message Group Index records, one per chunk in the file The attachment index records The statistics record The reader in this case will stop after the chunk index records. Using the channel records at the start of the read, the reader converts topic names to channel IDs. Using the chunk index records, the reader locates the chunks that must be read, based on the requested start times, channel IDs, and end times. These chunks will be a contiguous run. Readers may access the message data in at least two ways, \u201cfull scan\u201d: Seek from the chunk index to the start of the chunk using chunk_offset. Read/decompress the entire chunk, discarding messages not on the requested channels. Skip through the index data and into the next chunk if it is targeted too. \u201cindex scan\u201d: Consult the message_index_offsets field in the chunk index record, and use it to locate specific message indexes after the chunk for the channels of interest. These message indexes can be used to obtain a list of offsets, which the reader can seek to and extract messages from. Which of these options is preferable will tend to depend on the proportion of topics in use, as well as potentially whether the storage system is local or remote. Listing and accessing attachments The format provides the ability to list attachments contained within the file, and quickly extract them from the file contents. To list/select attachments in the file: Read the fixed-length footer and seek to the start of the index data section. Scan forward until encountering the attachment index, then read attachment index records until encountering a record that is not an attachment index. The records covered in the previous read will include attachment names, types, sizes, and timestamps. These can be used to fill out a list of attachments for selection. To select an attachment from the file, seek to the associated offset in the file and unpack the file content from the attachment record. Accessing summary statistics The format provides for fast local or remote access to summary information in the same style as \u201crosbag info\u201d, with the intent of functional parity with rosbag info. For reference, here is an example of the rosbag info output: path: demo.bag version: 2.0 duration: 7.8s start: Mar 21 2017 19:26:20.10 (1490149580.10) end: Mar 21 2017 19:26:27.88 (1490149587.88) size: 67.1 MB messages: 1606 compression: lz4 [79/79 chunks; 56.23%] uncompressed: 119.1 MB @ 15.3 MB/s compressed: 67.0 MB @ 8.6 MB/s (56.23%) types: diagnostic_msgs/DiagnosticArray [60810da900de1dd6ddd437c3503511da] radar_driver/RadarTracks [6a2de2f790cb8bb0e149d45d297462f8] sensor_msgs/CompressedImage [8f7a12909da2c9d3332d540a0977563f] sensor_msgs/PointCloud2 [1158d486dd51d683ce2f1be655c3c181] sensor_msgs/Range [c005c34273dc426c67a020a87bc24148] tf2_msgs/TFMessage [94810edda583a504dfda3829e70d7eec] topics: /diagnostics 52 msgs : diagnostic_msgs/DiagnosticArray /image_color/compressed 234 msgs : sensor_msgs/CompressedImage /radar/points 156 msgs : sensor_msgs/PointCloud2 /radar/range 156 msgs : sensor_msgs/Range /radar/tracks 156 msgs : radar_driver/RadarTracks /tf 774 msgs : tf2_msgs/TFMessage /velodyne_points 78 msgs : sensor_msgs/PointCloud2 The reader will recover this data from the index as follows: Read the fixed length footer and seek to the index_offset. Read the run of channel records that follow to get topic names, types, and MD5 data (which in case of ROS1 will be in the user data section), as well as channel IDs to interpret the chunk index records. After the channel are the chunk index records, if the file is chunked. From each chunk index record extract the compression algorithm and compressed/uncompressed size. From these the reader can compute the compression statistics shown in the rosbag info summary. For unchunked files this field is omitted. The MCAP version of \u201crosbag info\u201d will display information about included attachments as well. After reading the chunk index records, the attachment index records will be scanned and incorporated into the summary. Finally, the statistics record is used to compute the start, end, total, and per-channel message counts. The per-channel message counts must be grouped/summed over topics for display. The only difference between the chunked and unchunked versions of this output will be the chunk compression statistics (\u201ccompressed\u201d, \u201cuncompressed\u201d, \u201ccompression\u201d), which will be omitted in the case of unchunked files. The summary should be very fast to generate in either local or remote contexts, requiring no seeking around the file to visit chunks. The above is not meant to prescribe a summary formatting, but to demonstrate that parity with the rosbag summary is supported by MCAP. There are other details we may consider including, like references to per-channel encryption or compression if these features get uptake. We could also enable more interaction with the channel records, such as quickly obtaining schemas from the file for particular topics.","title":"Explanatory Notes"},{"location":"specification/explanatory-notes.html#explanatory-notes","text":"The following notes may be useful for users of the MCAP format, including implementers of readers and writers.","title":"Explanatory Notes"},{"location":"specification/explanatory-notes.html#feature-explanations","text":"The format is intended to support efficient, indexed reading of messages and generation of summary data in both local and remote contexts. \u201cSeeking\u201d should be imagined to incur either a disk seek or an HTTP range request to an object store \u2013 the latter being significantly more costly. In both random access and summarization, features may be unsupported due to choices taken by the writer of the file. For instance, statistics may not include channel message counts, or there may be no message index present. If the index data section is empty, the index_offset in the file footer will be set to zero.","title":"Feature Explanations"},{"location":"specification/explanatory-notes.html#scanning-for-records-on-specific-topics-within-an-interval","text":"The index is designed to support fast local and remote seek/filter operations with minimal seeking or range request overhead. The operation of the index for message reading is as follows: Client queries for all messages on topics /a, /b, /c between t0 and t1 Reader reads the fixed-length footer off the end of the file Reader parses the index_offset from the footer, and starts reading from that offset to the end of the file. During this read it will encounter the following in order: A run of Channel records, one per channel in the file A run of Message Group Index records, one per chunk in the file The attachment index records The statistics record The reader in this case will stop after the chunk index records. Using the channel records at the start of the read, the reader converts topic names to channel IDs. Using the chunk index records, the reader locates the chunks that must be read, based on the requested start times, channel IDs, and end times. These chunks will be a contiguous run. Readers may access the message data in at least two ways, \u201cfull scan\u201d: Seek from the chunk index to the start of the chunk using chunk_offset. Read/decompress the entire chunk, discarding messages not on the requested channels. Skip through the index data and into the next chunk if it is targeted too. \u201cindex scan\u201d: Consult the message_index_offsets field in the chunk index record, and use it to locate specific message indexes after the chunk for the channels of interest. These message indexes can be used to obtain a list of offsets, which the reader can seek to and extract messages from. Which of these options is preferable will tend to depend on the proportion of topics in use, as well as potentially whether the storage system is local or remote.","title":"Scanning for records on specific topics within an interval"},{"location":"specification/explanatory-notes.html#listing-and-accessing-attachments","text":"The format provides the ability to list attachments contained within the file, and quickly extract them from the file contents. To list/select attachments in the file: Read the fixed-length footer and seek to the start of the index data section. Scan forward until encountering the attachment index, then read attachment index records until encountering a record that is not an attachment index. The records covered in the previous read will include attachment names, types, sizes, and timestamps. These can be used to fill out a list of attachments for selection. To select an attachment from the file, seek to the associated offset in the file and unpack the file content from the attachment record.","title":"Listing and accessing attachments"},{"location":"specification/explanatory-notes.html#accessing-summary-statistics","text":"The format provides for fast local or remote access to summary information in the same style as \u201crosbag info\u201d, with the intent of functional parity with rosbag info. For reference, here is an example of the rosbag info output: path: demo.bag version: 2.0 duration: 7.8s start: Mar 21 2017 19:26:20.10 (1490149580.10) end: Mar 21 2017 19:26:27.88 (1490149587.88) size: 67.1 MB messages: 1606 compression: lz4 [79/79 chunks; 56.23%] uncompressed: 119.1 MB @ 15.3 MB/s compressed: 67.0 MB @ 8.6 MB/s (56.23%) types: diagnostic_msgs/DiagnosticArray [60810da900de1dd6ddd437c3503511da] radar_driver/RadarTracks [6a2de2f790cb8bb0e149d45d297462f8] sensor_msgs/CompressedImage [8f7a12909da2c9d3332d540a0977563f] sensor_msgs/PointCloud2 [1158d486dd51d683ce2f1be655c3c181] sensor_msgs/Range [c005c34273dc426c67a020a87bc24148] tf2_msgs/TFMessage [94810edda583a504dfda3829e70d7eec] topics: /diagnostics 52 msgs : diagnostic_msgs/DiagnosticArray /image_color/compressed 234 msgs : sensor_msgs/CompressedImage /radar/points 156 msgs : sensor_msgs/PointCloud2 /radar/range 156 msgs : sensor_msgs/Range /radar/tracks 156 msgs : radar_driver/RadarTracks /tf 774 msgs : tf2_msgs/TFMessage /velodyne_points 78 msgs : sensor_msgs/PointCloud2 The reader will recover this data from the index as follows: Read the fixed length footer and seek to the index_offset. Read the run of channel records that follow to get topic names, types, and MD5 data (which in case of ROS1 will be in the user data section), as well as channel IDs to interpret the chunk index records. After the channel are the chunk index records, if the file is chunked. From each chunk index record extract the compression algorithm and compressed/uncompressed size. From these the reader can compute the compression statistics shown in the rosbag info summary. For unchunked files this field is omitted. The MCAP version of \u201crosbag info\u201d will display information about included attachments as well. After reading the chunk index records, the attachment index records will be scanned and incorporated into the summary. Finally, the statistics record is used to compute the start, end, total, and per-channel message counts. The per-channel message counts must be grouped/summed over topics for display. The only difference between the chunked and unchunked versions of this output will be the chunk compression statistics (\u201ccompressed\u201d, \u201cuncompressed\u201d, \u201ccompression\u201d), which will be omitted in the case of unchunked files. The summary should be very fast to generate in either local or remote contexts, requiring no seeking around the file to visit chunks. The above is not meant to prescribe a summary formatting, but to demonstrate that parity with the rosbag summary is supported by MCAP. There are other details we may consider including, like references to per-channel encryption or compression if these features get uptake. We could also enable more interaction with the channel records, such as quickly obtaining schemas from the file for particular topics.","title":"Accessing summary statistics"}]}